{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66392104-5cae-4124-a943-21acc0edcfcd",
   "metadata": {},
   "source": [
    "# Derin Öğrenme\n",
    "- Derin Öğrenmeye Giriş\n",
    "- Kısa Tarihçe\n",
    "- Derin Öğrenme Alanındaki bazı uygulamalar\n",
    "- Derin öğrenmenin yaygınlaşmasının sebepleri\n",
    "- Derin öğrenme Bugüne kadar neler başardı\n",
    "- Derin öğrenme araçları\n",
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838dc6a1-9eac-4214-b240-090760357864",
   "metadata": {},
   "source": [
    "# Nedir\n",
    "- Andrew NG; \"Derin öğrenme bir süper güçtür. Bununla, bir bilgisayarın görmesini sağlayabilir, yeni bir sanat eserini sentezleyebilir, dilleri tercüme edebilir, tıbbi bir teşhis koyabilir veya kendi kendine gidebilecek bir arabanın parçalarını yapabilirsiniz. Bu bir süper güç değilse, ne olduğunu bilmiyorum\"\n",
    "- Derin Öğrenme insan beynin veri işleme ve karar verme şeklini referans alan bir makine öğrenmesi alt dalıdır.\n",
    "- Derin ifadesi yapay sinir ağlarında ki katmanları ifade eder.\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b02e47-cf82-41ab-8266-1f5f038570fe",
   "metadata": {},
   "source": [
    "# Kısa Tarihçe\n",
    "* 1950 - 2021 \n",
    "- Allan Turing- 1950- Bilgisayar ve Zeka- Turing testini oraya atmıştır, bu test bilgisayarın zekasını test etmemizi sağlar.\n",
    "- Jhın McCarthy- 1955- Turing testini geçebilecek bilgisayarlar için uygun yazılımlar yazılabileceğini inanıyorum, Yapay zeka kavramını ortaya atmıştır.\n",
    "- Perceptron- 1957 : Frank Rosemblat, öğrenme süreci sırasında beyin nöronlarının uyarlanmasından esinlenerek, bu kavramı ortaya çıkarmıştır.\n",
    "- Marvin Minsky- 1967 - Yapay sinir ağlarının herhangi bir keyfi öğrenme programına uygun nasıl otomatik olarak hayata gerçekleştirilebileceğini göstermiştir. \"Asimow ; benden zeki olduğunu inandığım 2 kişiden biridir, Minsky\". Minsky aynı zamanda, MIT'nin Yapay Zeka labavaratuarını kuran kişidir.\n",
    "- Deep Blue ve Kasparov- 1997- Deep Blue adlı bilgisayar Kasparov'u yenmeyi başarmıştır.\n",
    "- Deep Neural Network- 2006 - Sinir ağının çalışma prensiplerini derinleştirerek, birden fazla katman oluşturmak. Günümüzde derin öğrenme dendiği zaman, heğsi derindir.\n",
    "- Apple Siri ve Watson Jeopardy- 2011\n",
    "- Grafik İşlemcileri Çağı-2012- \"Geoff Hinton\", Large Scale Visual Recognetion adlı bir çalışmada, En iyi hata oranını yarı yarıya indirmiştir.\n",
    "- Amazon Alexa- İnsan düşünce süreçlerini yapılandırarak, teknolojiyi biraz daha ileriye taşımıştır. \n",
    "- GAN - 2014 - Belirli veri seti kullanılarak olmayan yeni yüzler oluşturulması.\n",
    "- AlphaGO-2016- Google Deepmind tarafından geliştirilen GO oyununu oynayan bir programdır. Avantaj verilmeden (Programa), profesyonel bir GO oyuncusuna karşı ilk defa oyun kazanan oyuncudur.\n",
    "- Google BERT- 2018- Doğal dil işleme ve  Metinler üzerinden öğrenme, Metinlerde ki anlamsal bilgileri öğrenme alanında çok önemli bir katkı.\n",
    "- GPT-2 - 2019 - \n",
    "- GPT-3 - 2020 - Derin öğrenmeyi kullanılabilen özbağlanımlı bir dil modelidir. yaklaşık 570 GB boyutunda ki metin ile beslenmiştir\n",
    "\n",
    "- DALL-E - 2021- Belirli tanımlar ve görseller üzerinden resim oluşturma.\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fc7a27-072e-4dba-9624-56ee93798477",
   "metadata": {},
   "source": [
    "# Derinin Öğrenmenin Yaygınlaşmasının Sebepleri\n",
    "* Ne kadar fazla veri olursa o kadar iyi.\n",
    "* Donanım ihtiyaçları karşılık bulabilmeye başladı.\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085fe031-a3d8-4d7d-b01c-28b14876e75a",
   "metadata": {},
   "source": [
    "# Derin öğrenme neler başardı\n",
    "* Neredeyse insan seviyesinde görüntü sınıflandırma.\n",
    "* Neredeyse insan seviyesinde ses tanıma\n",
    "* Neredeyse İnsan seviyesinde el yazısı tanıma\n",
    "* İnsan sevviyesine yakın otonom araç sürüşü\n",
    "* Sorulan sorulara cevaplar verbilmesi\n",
    "* Google Now ve Amazon Alexa gibi dijital yardımcıların hayat bulması\n",
    "* Çeviri araçlarının gelişmesi\n",
    "* Sentetik veri üretimi\n",
    "- ChatGPT...\n",
    "- Bard\n",
    "- Copilot...\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e49fdda-dc25-4651-91a9-34c5976284f3",
   "metadata": {},
   "source": [
    "# Derin öğrenme Araçları\n",
    "- TensorFlow- 2015 google.\n",
    "- Keras - François Chollet\n",
    "- PyTorch - facebook 2018\n",
    "- Scikit-learn - gpu desteği yok\n",
    "- OpenCV\n",
    "- Caffe\n",
    "- Knet.jl (Deniz Juliet Koç üni tarafından yazılmıştır, Julia ile yazılmıştır)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9467ee99-a656-49f7-addf-6c38607ae982",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Yapay Sinir Ağlarına Giriş\n",
    "* Motivasyon\n",
    "* Yapay Sinir Hücresi\n",
    "* Yapay Sinir Ağı\n",
    "* Yapay Sinir Ağları Nasıl Öğrenir ?\n",
    "* Gradyan İnişi (Gradient Descent)\n",
    "* Yapay Sinir Ağı Öğrenme Süreci\n",
    "* Geriye Yayılım(Backpropagation)\n",
    "* Update Kuralı\n",
    "* Aktivasyon Fonksiyonu (Activation Function)\n",
    "* Aşırı Öğrenme\n",
    "---\n",
    "### Motivasyon\n",
    "* Titanic örneğini konuştuk.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c37ba6c-d197-448e-823a-9afe3ba1747f",
   "metadata": {},
   "source": [
    "# Yapay Sinir Hücresi\n",
    "1. insan Sinir Hücresi\n",
    "* Dentirit : gelen sinyalleri somaya iletmek (ağırlıklarla alıyor)\n",
    "* Soma: Dendiritlerden gelen bilgiyi toplamak\n",
    "* Akson: toplanan bilgiyi hücrelere iletilmesi için aktarım yapar.\n",
    "* Sinapsis: bilgileri dönüşüme uğratmaktır ve daha sonra bilgileri hücrelere iletmektir.(değiştirilmiş ağırlıklarla gönderiyor)\n",
    "---\n",
    "* $x_0$ önceki nörünun aksonundan gelir, sinapsise. $w_0$ --- > $w_0 x_0$ ---> hücre gövdesine\n",
    "* $x_1$ önceki nörünun aksonundan gelir, sinapsise. $w_1$ --- > $w_1 x_1$ ---> hücre gövdesine\n",
    "* $x_2$ önceki nörünun aksonundan gelir, sinapsise. $w_2$ --- > $w_2 x_2$ ---> hücre gövdesine\n",
    "----- \n",
    "* hücre gövdesi\n",
    "### $\\sum{w_i x_i + b}$\n",
    "### activation funck.\n",
    "---\n",
    "* çıktı aksonu\n",
    "* $f(\\sum{w_i x_i + b})$\n",
    "--->\n",
    "* bir sonraki nöron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8f132e-eadf-437f-99a1-db01045c2653",
   "metadata": {},
   "source": [
    "# Yapay Sinir Ağı\n",
    "* Input-Hidden-Output layer\n",
    "* İnsan beyninin bilgi işleme şelşini referans alan sınıflandırma ve regresyon problemleri için kullanılabilen kuvvetli makine öğrenmesi algoritmalarından birisidir.\n",
    "* Frank Rosenblatt tarafından tanımlanmıştır.\n",
    "* Makinelere karar verebilme, kendi kendine öğrenebilme ve değerlendirebilme gibi yetkinlikleri kazandırarak insan beynini modellemeye çalışmaktır.\n",
    "* Derin sinir ağları, birden fazla gizli katman olduğunda verilen isimlendirme, aksi halde sığ sinir ağı (simple neural network).\n",
    "* Amaç : tahmin edilen değerler ile gerçek değerler arasında ki hatayı azaltmak bunun için birbirleri arasında ki ilişkiyi ifade ağırlıkların güncellenmesi bu amaca hizmet eder.\n",
    "----\n",
    "- François Chollet: Derin Sinir ağları bilgi damıtma ağları olarak düşünülebilir.\n",
    "-  Beyindeki tek bir nöron, bugün bile hala anlamlandıramadığımız inanılmaz derecede karmaşık bir makinedir. Bir sinir ağında ki tek bir 'nöron', biyolojik bir nöronun karmaşıklığının çok külçük bir bölümünü yakalayan inanılmaz derecede basit bir matematiksel işlevdir. - Andrew NG\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a48ee5-e1b2-47bc-8c47-64a00c6cc12b",
   "metadata": {},
   "source": [
    "# Yapay Sinir Ağları Nasıl Öğrenir?\n",
    "* matematiksel çalışma prensibi ve ağırlıkların etkilerini görsellerle anlattı adam. Yazıya dökmektense sade bir biçimde öğrenmeyi seçtim.\n",
    "* NASIL EN İYİ AĞIRLIK SETİNİ SEÇECEĞİZ --- GRADİENT DESCENT\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6664b59b-6aad-4a6e-a378-c908460c87ba",
   "metadata": {},
   "source": [
    "# GRADİENT DESCENT(Gradyan İnişi)\n",
    "* Fonksiyon minimizasyonu için kullanı\\lan bir optimizasyon yöntemidir.\n",
    "* $\\theta_j = \\theta_j - \\alpha \\frac{d}{d \\theta_j} J(\\theta) $\n",
    "* Cost, hata dağından en aşşağıya sıfıra inmeye çalışıyoruz.\n",
    "* Gradyanın(Türev işleminin sonucunun) negatifi olarak tanımlanan en dik iniş yönünde iteratif olarak parametre değerlini güncelleyerek ilgili fonksiyonun miinumum değerini verebilecek paramtereleri bulur.\n",
    "* cost : $  mse =  \\frac{1}{n} \\sum{y-\\hat{y}}$\n",
    "* Cost onksiyonunu Cost(b,w) u minimize edebilecek parametrelerini bulmak için kullanılır.\n",
    "---\n",
    "###  Peki ama Nasıl ?\n",
    "* Paramtre değerlini iterartif olarak cost'u azaltacak şekilde değiştirerek yapar.\n",
    "* Bir fonksiyonun bir noktada ki türevi o fonksiyonun maksimum artış yönünü verir.\n",
    "* Cost fonksiyonunun bir noktada ki (w) türevi cost fonk. maks. artış yönünü verir.\n",
    "* bu yönün tersine gidildiğinde maksimum azaldığı yöne gidilmiş olur.\n",
    "* Yani w'ya göre kısmi türev alındığında cost'u en fazla arttıracak yön bilgisi bulunur.\n",
    "* Bu yönün tersine gidildiğinde cost'un en hızlı azalacağı tarafa gidilmiş olur.\n",
    "* Belirli bir gidiş hızında (learning rate)  ve belirli bir iterasyon sayısınca bu işlem tekrar edilir.\n",
    "---\n",
    "### Matematiksel olarak nasıl gerçekleşir.\n",
    "* $ J(\\theta_0 \\theta_1) = \\frac {1}{2m} \\sum{h_0((X^i)- y^i)^2}$\n",
    "* $minimizeJ(\\theta_0,\\theta_1)$\n",
    "* $\\theta_0 ve \\theta_1$ e göre kısmi türevler alınır.\n",
    "##  $ \\frac{d}{d \\theta_0} \\frac {1}{2m}(\\sum{(\\theta_0+\\theta_1 X^i - y^i)^2}) = \\frac{1}{m} \\sum{h_0((X^i)- y^i)}$\n",
    "##  $ \\frac{d}{d \\theta_1} \\frac {1}{2m}(\\sum{(\\theta_0+\\theta_1 X^i - y^i)^2}) = \\frac{1}{m} \\sum{h_0((X^i)- y^i)} . x^i $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934e0e74-86d5-41de-a496-d1c3e4f31a01",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Yapay Sinir Ağı Öğrenme Süreci\n",
    "* Ağaırlık ve Bias değerlerinin başlatılması, ileriye Yayılım ---> Hata ---> Geriye Yayılım(back prop..) ---> Paramterelerinin Güncellenmesi ---> İleriye yayılım .... (belirli bir iterasyon(epoch) sayısınca) --> Final Tahmin\n",
    "* Aşırı öğrenmeye düşmeden, test setimizde hata oranın en düşük olduğu noktaya kadar devam ediyoruz, overffiting (aşırı öğrenme olmasını istemiyoruz)\n",
    "-----\n",
    "# Sinir Ağı Mimarisi Tasarlama\n",
    "1. $i_1 , i_2$ (input)\n",
    "2. $h_1, h_2$ (hidden layer, nöron), $b_1, b_2$ bias değerleri  \n",
    "3. $o_1, o_2$ (output)\n",
    "---\n",
    "* rastgele bias ve ağırlık değerleri verilir ilk başta\n",
    "* aktivasyon fonksiyonu (sigmoid)\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d925f1-8c38-43d0-94d7-9221d17f5cde",
   "metadata": {},
   "source": [
    "# İleriye yayılım (Forward Propogation)\n",
    "##  $net_{h_1} = w_1 * i_1+w_2* i_2 + b_1*1$\n",
    "##  $net_{h_1} = 0.15 * 0.05+ 0.2* 0.1 + 0.35 * 1 = 0.3775$\n",
    "## $out_{h_1} = \\frac{1}{1+e^(net_{h_1})}=  \\frac{1}{1+e^(3775)}= 0.5933$\n",
    "## $out_{h_2} = 0.5969$\n",
    "-----\n",
    "##  $net_{o_1} = w_1 * i_1+w_2* i_2 + b_1*1$\n",
    "##  $net_{o_1} = 0.4 * 0.5933+ 0.45* 0.5969 + 0.6 * 1 = 1.1059$\n",
    "## $out_{o_1} = \\frac{1}{1+e^(net_{o_1})}=  \\frac{1}{1+e^(1.1059)}= 0.7514$\n",
    "## $out_{o_2} = 0.7729$\n",
    "---\n",
    "##  $e_{\\text{total}} = \\frac{1}{2} \\sum_{i}(y_i - \\hat{y}_i)^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20ac79c-9258-4051-ad5a-1c48af0c2980",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Geriye Yayılım (Back Propogation)\n",
    "* Geriye Yayılım sinir ağlarında ağırlık optimizasyonu için kullanulan bir yöntemdir.\n",
    "* Amacımız gerçek değerler ile tahmin edilen değerler arasında ki farkı ifade eden error/loss'u minimize etmeye çalışmak.\n",
    "* Gradient Descent'i kullanarak bu optimizasyon işlemini gerçekleştirir.\n",
    "### $ \\frac{d e_t} {d w_5} = ? $ toplam hatanın $w_5$ e göre kısmi türevini almamız lazım\n",
    "* direk hesap edemeyiz çünkü w5'e $e_t den$ direk olarak ulaşamayız.\n",
    "* Bunun için zincir çözümü yapacağız.\n",
    "\n",
    "### $\\frac{d e_t}{d w_5} = \\frac{d e_t}{d out_{o_1}} \\cdot \\frac{d out_{o_1}}{d net_{o_1}} \\cdot \\frac{d net_{o_1}}{d w_5}$\n",
    "\n",
    "### $\\frac{d e_t}{d out_{o_1}} = ? $\n",
    "\n",
    "### $e_{\\text{total}} =\\frac{1}{2} (y_{o_1}-out_{o_1})^2 + \\frac{1}{2}(y_{o_2}-out_{o_2})^2  $\n",
    "### $\\frac{d e_t}{d out_{o_1}} = -(y_{o_1}-out_{o_1}) = -(0.01 - 0.7513)= 0.7413$\n",
    "--\n",
    "\n",
    "### $\\frac{d out_{o_1}}{d net_{o_1}}$\n",
    "...\n",
    "### $ out_{o_1} = \\frac{1}{1+ e^-(net_{o_1})}  = 0.7514 $  (sigmoid bu arada bu ! daha önce yazmamıştım herhalde)\n",
    "\n",
    "### $\\frac{d out_{o_1}}{d net_{o_1}} = out_{o_1} (1-out_{o_1} = 0.7513 \\cdot (1-0.7513))$\n",
    "...\n",
    "### $\\frac{d net_{o_1}}{d w_5}= ?$\n",
    "\n",
    "###$net_{0_1} = w_5 * out_{h_1}+w_6* out_{h_2} + b_2*1$\n",
    "\n",
    "### $\\frac{d net_{o_1}}{d w_5}= 1 * out_{h_1}= 0.5932$\n",
    "--- \n",
    "\n",
    "### $\\frac{d e_t}{d w_5} = \\frac{d e_t}{d out_{o_1}} \\cdot \\frac{d out_{o_1}}{d net_{o_1}} \\cdot \\frac{d net_{o_1}}{d w_5}$\n",
    "$\\frac{d e_t}{d w_5} = 0.7413 *  0.7514 *  0.5932 = 0.0821 $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b064c279-94a9-47bb-ac0b-2569e15e584e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Update Kuralı : ağırlık güncelleme\n",
    "*  $ w_5 = w_5 - \\alpha -\\frac{d e_t}{d w_5}$  (burada ki eksi ifadesi aslında gradient descent i bizim amacımıza uyarlayan bölümdür ! )\n",
    "..\n",
    "* alpha işareti, learning rate'i temsil eder, öğrenme hızı, adımların büyüklüğünü kontrol ediyoruz. deneme yanılma yöntemi ile idealini bulmamız gereken bir hiper parametredir.\n",
    "* $w_5 = 0.4 - 0.5 * 0.0821$\n",
    "..\n",
    "* $w_5 = 0.3598$\n",
    "\n",
    "* Bu işleme 'Yapay Zeka Öğreniyor' dendiği kısımdır."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b3ab1d-1b24-4577-aa6f-b9f29f023491",
   "metadata": {},
   "source": [
    "# Aktivasyon Fonksiyonu\n",
    "* Nörondan gelen net bilgiyi bir değişmden geçirerek, diğer bir nörona geçirir.\n",
    "* Karmaşık yapıda ki veriler içerisinden yapay sinir ağlarının öğrenmesini sağlayabilmek adına kullanılan fonksiyonlara aktivasyon fonksiyonları denir.\n",
    "---\n",
    "1. Sigmoid : en yaygın kullanılan aktivasyonlardan birisidir. döndürdüğü değer 0-1 arasıdır. İkili sınıflandırma problemlerinde kullanılır. Türevinin de görünümden anlaşılacağı üzere, uç değerler girildiğinde 0'a yakınsar bu sebeple kaybolan gradyan (vanishing gradian) problemine yol açar. Geriye Yayılım sırasında öğrenimin durmasına sebep olur. Bu sebeple gizli katmanlarda tercih edilmez. \n",
    "### $\\sigma(x) = \\frac{1}{1 + e^{-x}}$\n",
    "### $\\sigma'(x) = \\sigma(x) \\cdot (1 - \\sigma(x)) $\n",
    "2. Hiperbolik tanjant : -1 ile +1 arasında değer üreten, doğrusal olmayan bir fonksiyondur. ikili sınıflandırma problemlerinde kullanılır. uç noktalarda türevi 0'a yakınsar yani back propogation esnasında öğrenim durur.\n",
    "### $\\tanh(x) = \\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}$\n",
    "### $\\tanh'(x) = 1 - \\tanh^2(x)$\n",
    "3. ReLu (rectified linear unit) : doğrultulmuş lineer birim , doğrusal olmayan bir fonk, negaitf girdiler için 0 değerini alırken, pozitifler için çeşitli değerler alır. öğrenme açısından daha verimli ve hızlı olmayı sağlar. \n",
    "### $\\text{ReLU}(x) = \\max(0, x)$\n",
    "### $\\text{ReLU}'(x) = \\begin{cases} 0, & \\text{if } x < 0 \\\\1, & \\text{if } x \\geq 0 \\end{cases}$\n",
    "----\n",
    "* gizli katmanlarda genelde ReLu kullanılır, çıktı katmanında ise 2 sınıflı bir sınıflandırma bir söz konusu ise sigmoid, çok sınıflı bir sınıflandırma söz konusu ise softmax isimli aktivasyon fonksiyonları kullanılır.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90e66f0-e559-4151-a78f-f958865e7d58",
   "metadata": {},
   "source": [
    "# Overfitting (Aşırı öğrenme)\n",
    "* çok öğrenme , ezberlemek.\n",
    "* Early Stopping\n",
    "* $L_1$ $L_2$ düzenlileştirme\n",
    "* Sönümleme (Dropout)\n",
    "* Veri Artırımı (Data Augmentation)\n",
    "---\n",
    "* Modelimizin Veriyi Aşırı öğrenmesi, birebir öğrenmesi,  over fitting YÜKSEK VARYANS\n",
    "* Doğru model , Düşük yanlılık, düşük varyans\n",
    "* Underfitting Yüksek yanlılık\n",
    "---- \n",
    "### Erken durdurma\n",
    "- Test setinde ki mse düşmeyi bıraktığı zaman eğitimi durdurmak.\n",
    "----\n",
    "### $L_1$ $L_2$ düzenlileştirme\n",
    "* Katsayılara ceza uygulamak \n",
    "* $L_1$ : $ Loss + \\lambda \\sum{|w_j|}$\n",
    "* $ L_2 : Loss +  \\lambda \\sum{(w_j)^2}$\n",
    "---\n",
    "### Dropou (Sönümleme)\n",
    "* Sinir ağı içerisinde yer alan nöronların kullanıcı tarafından belirlenen bir oranda rastgele olarak söndürülmesiyle aşırı öğrenmeyi önleyerek, performansın artırılması amaçlanmıştır.\n",
    "---\n",
    "### Veri Artırımı\n",
    "* Veri artırımı veri setinde bulunan verilere çeşitli özellikler ekleyerk veri miktarını artırmayı sağlayan bir teknkitir."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
