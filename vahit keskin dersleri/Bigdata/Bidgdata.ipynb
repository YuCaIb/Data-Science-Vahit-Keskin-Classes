{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dac13c8-7bee-43f5-af12-3d3980a7580f",
   "metadata": {},
   "source": [
    "# Big Data\n",
    "---\n",
    "* Biz(insanoğlu) veriyi üreten kaynağın kendisiyiz. Aynı zamanda ürüyen bu veriyide anlamdırmak zorunda olduğumuz bir konumda bulunuyoruz.\n",
    "* Veri üretimi, parmak izimiz üstel bir şekilde artıyor. Korkunç boyutlarda!\n",
    "---\n",
    "## Tanım:  Veri İşlemeye yeni bir vizyon ! Geleneksel yöntemlerle işlenemeyen verilere büyük veri denir ! \n",
    "* Geleneksel yöntemlerle işlenememe durumu; bazen verinin boyutu, bazen verinin türü ve bazen de verinin hızlı bir şekilde değişebiliyor olmasıdır.\n",
    "* mesela çok büyük boyutlarda bir veri seti olsun, bir sql sorgusu gönderdiğimizde saatlerce belki de hiç bir zaman o sorgu geri dönmeyebiliyordu. bu geleneksel yöntemlerle işlenememe durumunu ifade eden olaydır.\n",
    "* Bu problemin çözümü birden fazla bilgisayarın bir araya gelerek tek bir bilgisayar gibi hareket etmesidir. Yani bir işi yapmak için birden fazla bilgisayarın tek bir bilgisayar gibi davranmasıdır.\n",
    "---\n",
    "#### Problemin Çözümünün Bize Kazandırdıkları\n",
    "* Veri analitiği alanda yeni ufuklar açtı, hesaplama gücünün artmasıyla makine öğrenmesi algoritmalarının performansı arttı. Daha bbüyük miktar ve çeşitteki verilerin kullanılması ile veriden faydalı bilgi çıkarmak süreci için çok önemli bir kaynak/araç sağlanmış oldu.\n",
    "-----\n",
    "### Büyük Verinin Bileşenleri\n",
    "* Ortaya çıkan veri işleme güçlüğü verinin hacmi, çeşitliliği ve hızı ile alakalıdır. (Velocity,Variety,Volume)\n",
    "* Büyük veriyi ifade eden özellikler hacim,çeşitlilik ve hızdır.\n",
    "* Çok yaygın bir yanılgı bu özelliklerin birlikte olduğunda ancak veriye büyük veri denebileceğidir. ! Böyle bir durum söz konusu değil.\n",
    "* en odaklanacağımız nokta hacim olacak ! \n",
    "* Büyük veri araçları veriden bilgi çıkarma süreçleri için çok güçlü bir araçtır.\n",
    "----\n",
    "## Amacımız hep aynı: VERİDEN FAYDALI BİLGİ ÇIKARMAK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c017e76a-f969-4cae-a5f1-fd7d56609581",
   "metadata": {},
   "source": [
    "# APACHE HADOOP\n",
    "* Apache Hadoop açık kaynak kodlu, güvenilir, ölçeklenebilir paralel hesaplama yazılımı projesidir.\n",
    "* Bütük veri teknolojilerinin temeleini oluşturur\n",
    "* Geleneksel yöntemler ile etkin olarak işleenmesi mümkün olmayan verilerin işlenebilmesine olanak sağlamaktadır.\n",
    "* Bir bilgisayar kümesinin belirli bir işi yapmak için tek bir bilgisayar gibi birlikte haraket etmesini sağlamaktadır.\n",
    "---\n",
    "#### Apache Hadoop bBileşenleri:\n",
    "* Hadoop Common: Büyük veri teknolojileri, diğer bir ifadesi ile tüm hadoop modüllerini destekleyen ortak gereksinimlerdir.\n",
    "* Hadoop Dağıtık Dosya Sistemi(HDFS) : Yerel dosya sistemleri olan FAT32 ve NFTS gibi bir dosya sistemidir. Bu sistemlerden farkı büyük boyutlardaki veriyi dağıtık şekilde depolamaya ve kontrol etmeye imkan sağlamaktadır.\n",
    "* Hadoop YARN: Kaynak yönetimi ve iş planlaması için kullanılan Apache Hadoop'un daha etkin bir şekilde kullanılmasına olanak sağlayan bir bileşendir.\n",
    "* Hadoop MadReduce: Büyük veri dünyasının fonksiyonel anlamda temelini oluşturan bileşendir. Aynı ağdaki dağıtık bilgisayar kümeleri üzerinde büyük veri analizi yapılabilmesi için geliştirilmiş bir programlama modelidir.\n",
    "------\n",
    "* Hadoop Küme Yapısı: HadoopMaster[Name Node][ Resource Manager]-Hadoop Slave1[Data Node][Node Menager]-Hadoop Slave2[Data Node][Node Menager]\n",
    "--------\n",
    "### MapReduce Nedir?\n",
    "* MapREduce : Mapping,Shuffiling,Reducing.\n",
    "* Mapping : Map aşamasının görevi HDFS üzerinde ki bilgiyi işlemektir. Veriyi işler yeni veri parçaları oluşturur.\n",
    "* Shuffle ve Reduce : Shuffle ve Reduce aşamalarının görevi Map aşamasından gelen veriyi işlemek ve indirgemektir.\n",
    "* Merge: Parçalanan veriyi bir araya getirmek ! .\n",
    "---\n",
    "## APache Hadoop Faydaları\n",
    "* Veri saklama ve işleme gücü\n",
    "* Açık kaynak \n",
    "* Hız\n",
    "* Esneklik\n",
    "* Ölçeklenebilirlik\n",
    "* Hata Toleransı\n",
    "---\n",
    "# Apache Hadoop Çözüm olarak yeterli mi ?\n",
    "* Disk tabanlı çalışan bir modeldir.\n",
    "* Her MapReduce görevinde diskten okuma diske yazma işlemi yapılır.\n",
    "* İteratif işlemler zaman alır ve kaynakları meşgul eder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7531e560-3f72-4587-813d-44d7f50f6b34",
   "metadata": {},
   "source": [
    "# APACHE SPARK\n",
    "* Apache Spark küme üzerinde hızlı ve genel amaçlı bilgi işleme sistemidir.\n",
    "* MapReduce modelinde yer alan disk bazlı çalışma sisteminin yarattığı maliyetlerden dolayı ortaya çıkmıştır.\n",
    "* Apache Hadoop'a göre 100 kat daha hızlı çalışmaktadır.\n",
    "* Java, Scala, Python ve R ile uygulama geliştirilebilir.\n",
    "* Genelleştiricidir. Spark SQL, Spark MLlib, Spark Streaming, GraphX aynı uygulama içinde kullanılabilir.\n",
    "---\n",
    "### Apache Spark Bileşenleri:\n",
    "* Spar Core ve RDD'S; Hafıza yönetimi, Görevlerin Dağıtılması, Hata Kurtarma, Dosya Sistemlerine erişim.\n",
    "* RDD's = MapReduce'un alternatifidir. Apache Spark'ın programlama modeli verinin bellek içi tutularak paralel işlenmesini ifade etmektedir ve bu da RDD's dir.\n",
    "* Spark SQL ; SQL ya da bir Dataframe API kullanarak  Spark programlarında yapılandırılmış veriyi sorgulama imkanı sağlar.\n",
    "* Spark MLlib: Apache Spark'ın ölçeklenebilir makine öğrenmesi kütphanesidir. Apache Spark'ın bellek içi çalışma mantığından dolayı iteratif işlemler barındıran, makine öprenmesi gibi işlemlerde bize çözüm sunmaktadır.\n",
    "* Spark Streaming: Akan verinin ölçeklenebilir, yüksek hacimli ev hata toleranslı bir şekilde işlenmesine olanak sağlayan temel Spark API'sine ait bir spark uzantısıdır.\n",
    "* Graph X: Paralel grafik tabanlı hesaplama işlemleri için kullanılan bir kütphanedir.\n",
    "-----\n",
    "### Dayanık Dağıtık Data Setler (RDD'S)\n",
    "* RDD'de yapılan işlem veriyi RAM'e taşımak, RAM üzerinde dönüşüm işlemlerini ve gerekli iteratif işlemleri yapmak ve en son işlemler bittikten sonra veriyi tekrar diske yazmaktır.\n",
    "* \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61e1e00-985b-43b8-8911-eefc3e03229f",
   "metadata": {},
   "source": [
    "# Ekosistemin diğer Bazı Üyeleri:\n",
    "* "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
