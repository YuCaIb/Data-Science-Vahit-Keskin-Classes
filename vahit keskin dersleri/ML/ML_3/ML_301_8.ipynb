{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bb37c1c-8bf3-4a31-8e03-8d0d90197e19",
   "metadata": {},
   "source": [
    "# Light GBM (Microsoft 2017)\n",
    "* XGBoost'un eğitim süresi performansını arttırmaya yönelik geliştirilen bir diğer GBM Türüdür.\n",
    "* Daha performanslı\n",
    "* Level-wise büyüme stratejisi yerine Leaf-wise büyüme stratejisi\n",
    "* Breadth-first search (BFS) yerine, depth-first search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02781490-2687-442b-8c0b-7a9fc4991e39",
   "metadata": {},
   "source": [
    "libraray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7746ba37-0e56-4b91-a528-dbbc595e31e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import scale, StandardScaler ## standartlaStırma\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn import neighbors\n",
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39201cad-f63f-4d2d-9900-56f981f8c53c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('Hitters.csv')\n",
    "df = df.dropna()\n",
    "y= df['Salary']\n",
    "dms = pd.get_dummies(df[['League', 'Division', 'NewLeague']]) ## one hot encoding\n",
    "X_= df.drop(['Salary', 'League', 'Division','NewLeague'], axis =1).astype('float64')\n",
    "X= pd.concat([X_,dms[['League_N', 'Division_W', 'NewLeague_N']]], axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1a1fa9-8529-4b16-84a2-8a2a702cee21",
   "metadata": {},
   "source": [
    "### Model ve tahmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ea413b1-47d0-448e-96ad-924fc3e9cfa5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lightgbm\n",
      "  Obtaining dependency information for lightgbm from https://files.pythonhosted.org/packages/74/02/5ad29a2a3d193a87d5a05fb7fd3b4e30b8eb6db7a9ddbe193beb7053978f/lightgbm-4.2.0-py3-none-win_amd64.whl.metadata\n",
      "  Downloading lightgbm-4.2.0-py3-none-win_amd64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\ycanf\\anaconda3\\lib\\site-packages (from lightgbm) (1.24.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\ycanf\\anaconda3\\lib\\site-packages (from lightgbm) (1.10.1)\n",
      "Downloading lightgbm-4.2.0-py3-none-win_amd64.whl (1.3 MB)\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.0/1.3 MB 59.5 kB/s eta 0:00:22\n",
      "    --------------------------------------- 0.0/1.3 MB 59.5 kB/s eta 0:00:22\n",
      "    --------------------------------------- 0.0/1.3 MB 59.5 kB/s eta 0:00:22\n",
      "    --------------------------------------- 0.0/1.3 MB 59.5 kB/s eta 0:00:22\n",
      "    --------------------------------------- 0.0/1.3 MB 59.5 kB/s eta 0:00:22\n",
      "    --------------------------------------- 0.0/1.3 MB 59.5 kB/s eta 0:00:22\n",
      "   - -------------------------------------- 0.0/1.3 MB 49.2 kB/s eta 0:00:27\n",
      "   - -------------------------------------- 0.0/1.3 MB 49.2 kB/s eta 0:00:27\n",
      "   - -------------------------------------- 0.0/1.3 MB 49.2 kB/s eta 0:00:27\n",
      "   - -------------------------------------- 0.0/1.3 MB 49.2 kB/s eta 0:00:27\n",
      "   - -------------------------------------- 0.0/1.3 MB 49.2 kB/s eta 0:00:27\n",
      "   - -------------------------------------- 0.0/1.3 MB 49.2 kB/s eta 0:00:27\n",
      "   - -------------------------------------- 0.0/1.3 MB 49.2 kB/s eta 0:00:27\n",
      "   - -------------------------------------- 0.1/1.3 MB 57.5 kB/s eta 0:00:23\n",
      "   - -------------------------------------- 0.1/1.3 MB 57.5 kB/s eta 0:00:23\n",
      "   - -------------------------------------- 0.1/1.3 MB 57.5 kB/s eta 0:00:23\n",
      "   - -------------------------------------- 0.1/1.3 MB 57.5 kB/s eta 0:00:23\n",
      "   - -------------------------------------- 0.1/1.3 MB 57.5 kB/s eta 0:00:23\n",
      "   - -------------------------------------- 0.1/1.3 MB 57.5 kB/s eta 0:00:23\n",
      "   - -------------------------------------- 0.1/1.3 MB 57.5 kB/s eta 0:00:23\n",
      "   -- ------------------------------------- 0.1/1.3 MB 59.6 kB/s eta 0:00:22\n",
      "   -- ------------------------------------- 0.1/1.3 MB 59.6 kB/s eta 0:00:22\n",
      "   -- ------------------------------------- 0.1/1.3 MB 59.6 kB/s eta 0:00:22\n",
      "   -- ------------------------------------- 0.1/1.3 MB 59.6 kB/s eta 0:00:22\n",
      "   -- ------------------------------------- 0.1/1.3 MB 59.6 kB/s eta 0:00:22\n",
      "   -- ------------------------------------- 0.1/1.3 MB 59.6 kB/s eta 0:00:22\n",
      "   -- ------------------------------------- 0.1/1.3 MB 59.6 kB/s eta 0:00:22\n",
      "   -- ------------------------------------- 0.1/1.3 MB 54.0 kB/s eta 0:00:24\n",
      "   -- ------------------------------------- 0.1/1.3 MB 54.0 kB/s eta 0:00:24\n",
      "   -- ------------------------------------- 0.1/1.3 MB 54.0 kB/s eta 0:00:24\n",
      "   -- ------------------------------------- 0.1/1.3 MB 54.0 kB/s eta 0:00:24\n",
      "   -- ------------------------------------- 0.1/1.3 MB 54.0 kB/s eta 0:00:24\n",
      "   -- ------------------------------------- 0.1/1.3 MB 54.0 kB/s eta 0:00:24\n",
      "   --- ------------------------------------ 0.1/1.3 MB 57.5 kB/s eta 0:00:22\n",
      "   --- ------------------------------------ 0.1/1.3 MB 57.5 kB/s eta 0:00:22\n",
      "   --- ------------------------------------ 0.1/1.3 MB 57.5 kB/s eta 0:00:22\n",
      "   --- ------------------------------------ 0.1/1.3 MB 57.5 kB/s eta 0:00:22\n",
      "   --- ------------------------------------ 0.1/1.3 MB 57.5 kB/s eta 0:00:22\n",
      "   --- ------------------------------------ 0.1/1.3 MB 57.5 kB/s eta 0:00:22\n",
      "   --- ------------------------------------ 0.1/1.3 MB 57.5 kB/s eta 0:00:22\n",
      "   --- ------------------------------------ 0.1/1.3 MB 53.8 kB/s eta 0:00:23\n",
      "   --- ------------------------------------ 0.1/1.3 MB 53.8 kB/s eta 0:00:23\n",
      "   --- ------------------------------------ 0.1/1.3 MB 53.8 kB/s eta 0:00:23\n",
      "   --- ------------------------------------ 0.1/1.3 MB 53.8 kB/s eta 0:00:23\n",
      "   --- ------------------------------------ 0.1/1.3 MB 53.8 kB/s eta 0:00:23\n",
      "   --- ------------------------------------ 0.1/1.3 MB 53.8 kB/s eta 0:00:23\n",
      "   --- ------------------------------------ 0.1/1.3 MB 53.8 kB/s eta 0:00:23\n",
      "   ---- ----------------------------------- 0.1/1.3 MB 55.3 kB/s eta 0:00:22\n",
      "   ---- ----------------------------------- 0.1/1.3 MB 55.3 kB/s eta 0:00:22\n",
      "   ---- ----------------------------------- 0.1/1.3 MB 55.3 kB/s eta 0:00:22\n",
      "   ---- ----------------------------------- 0.1/1.3 MB 55.3 kB/s eta 0:00:22\n",
      "   ---- ----------------------------------- 0.1/1.3 MB 55.3 kB/s eta 0:00:22\n",
      "   ---- ----------------------------------- 0.1/1.3 MB 55.3 kB/s eta 0:00:22\n",
      "   ---- ----------------------------------- 0.1/1.3 MB 55.3 kB/s eta 0:00:22\n",
      "   ---- ----------------------------------- 0.2/1.3 MB 56.5 kB/s eta 0:00:21\n",
      "   ---- ----------------------------------- 0.2/1.3 MB 56.5 kB/s eta 0:00:21\n",
      "   ---- ----------------------------------- 0.2/1.3 MB 56.5 kB/s eta 0:00:21\n",
      "   ---- ----------------------------------- 0.2/1.3 MB 56.5 kB/s eta 0:00:21\n",
      "   ---- ----------------------------------- 0.2/1.3 MB 56.5 kB/s eta 0:00:21\n",
      "   ---- ----------------------------------- 0.2/1.3 MB 56.5 kB/s eta 0:00:21\n",
      "   ---- ----------------------------------- 0.2/1.3 MB 56.5 kB/s eta 0:00:21\n",
      "   ----- ---------------------------------- 0.2/1.3 MB 54.6 kB/s eta 0:00:22\n",
      "   ----- ---------------------------------- 0.2/1.3 MB 54.6 kB/s eta 0:00:22\n",
      "   ----- ---------------------------------- 0.2/1.3 MB 54.6 kB/s eta 0:00:22\n",
      "   ----- ---------------------------------- 0.2/1.3 MB 54.6 kB/s eta 0:00:22\n",
      "   ----- ---------------------------------- 0.2/1.3 MB 54.6 kB/s eta 0:00:22\n",
      "   ----- ---------------------------------- 0.2/1.3 MB 54.6 kB/s eta 0:00:22\n",
      "   ----- ---------------------------------- 0.2/1.3 MB 54.6 kB/s eta 0:00:22\n",
      "   ----- ---------------------------------- 0.2/1.3 MB 55.7 kB/s eta 0:00:21\n",
      "   ----- ---------------------------------- 0.2/1.3 MB 55.7 kB/s eta 0:00:21\n",
      "   ----- ---------------------------------- 0.2/1.3 MB 55.7 kB/s eta 0:00:21\n",
      "   ----- ---------------------------------- 0.2/1.3 MB 55.7 kB/s eta 0:00:21\n",
      "   ----- ---------------------------------- 0.2/1.3 MB 55.7 kB/s eta 0:00:21\n",
      "   ----- ---------------------------------- 0.2/1.3 MB 55.7 kB/s eta 0:00:21\n",
      "   ----- ---------------------------------- 0.2/1.3 MB 55.7 kB/s eta 0:00:21\n",
      "   ------ --------------------------------- 0.2/1.3 MB 53.7 kB/s eta 0:00:22\n",
      "   ------ --------------------------------- 0.2/1.3 MB 53.7 kB/s eta 0:00:22\n",
      "   ------ --------------------------------- 0.2/1.3 MB 53.7 kB/s eta 0:00:22\n",
      "   ------ --------------------------------- 0.2/1.3 MB 53.7 kB/s eta 0:00:22\n",
      "   ------ --------------------------------- 0.2/1.3 MB 53.7 kB/s eta 0:00:22\n",
      "   ------ --------------------------------- 0.2/1.3 MB 53.7 kB/s eta 0:00:22\n",
      "   ------ --------------------------------- 0.2/1.3 MB 53.7 kB/s eta 0:00:22\n",
      "   ------ --------------------------------- 0.2/1.3 MB 55.3 kB/s eta 0:00:21\n",
      "   ------ --------------------------------- 0.2/1.3 MB 55.3 kB/s eta 0:00:21\n",
      "   ------ --------------------------------- 0.2/1.3 MB 55.3 kB/s eta 0:00:21\n",
      "   ------ --------------------------------- 0.2/1.3 MB 55.3 kB/s eta 0:00:21\n",
      "   ------ --------------------------------- 0.2/1.3 MB 55.3 kB/s eta 0:00:21\n",
      "   ------ --------------------------------- 0.2/1.3 MB 55.3 kB/s eta 0:00:21\n",
      "   ------ --------------------------------- 0.2/1.3 MB 55.3 kB/s eta 0:00:21\n",
      "   ------- -------------------------------- 0.2/1.3 MB 56.2 kB/s eta 0:00:20\n",
      "   ------- -------------------------------- 0.2/1.3 MB 56.2 kB/s eta 0:00:20\n",
      "   ------- -------------------------------- 0.2/1.3 MB 56.2 kB/s eta 0:00:20\n",
      "   ------- -------------------------------- 0.2/1.3 MB 56.2 kB/s eta 0:00:20\n",
      "   ------- -------------------------------- 0.2/1.3 MB 56.2 kB/s eta 0:00:20\n",
      "   ------- -------------------------------- 0.2/1.3 MB 56.2 kB/s eta 0:00:20\n",
      "   ------- -------------------------------- 0.2/1.3 MB 56.2 kB/s eta 0:00:20\n",
      "   ------- -------------------------------- 0.3/1.3 MB 54.8 kB/s eta 0:00:20\n",
      "   ------- -------------------------------- 0.3/1.3 MB 54.8 kB/s eta 0:00:20\n",
      "   ------- -------------------------------- 0.3/1.3 MB 54.8 kB/s eta 0:00:20\n",
      "   ------- -------------------------------- 0.3/1.3 MB 54.8 kB/s eta 0:00:20\n",
      "   ------- -------------------------------- 0.3/1.3 MB 54.8 kB/s eta 0:00:20\n",
      "   ------- -------------------------------- 0.3/1.3 MB 54.8 kB/s eta 0:00:20\n",
      "   -------- ------------------------------- 0.3/1.3 MB 55.9 kB/s eta 0:00:20\n",
      "   -------- ------------------------------- 0.3/1.3 MB 55.9 kB/s eta 0:00:20\n",
      "   -------- ------------------------------- 0.3/1.3 MB 55.9 kB/s eta 0:00:20\n",
      "   -------- ------------------------------- 0.3/1.3 MB 55.9 kB/s eta 0:00:20\n",
      "   -------- ------------------------------- 0.3/1.3 MB 55.9 kB/s eta 0:00:20\n",
      "   -------- ------------------------------- 0.3/1.3 MB 55.9 kB/s eta 0:00:20\n",
      "   ---------- ----------------------------- 0.4/1.3 MB 69.2 kB/s eta 0:00:15\n",
      "   ---------- ----------------------------- 0.4/1.3 MB 69.2 kB/s eta 0:00:15\n",
      "   ----------- ---------------------------- 0.4/1.3 MB 70.4 kB/s eta 0:00:14\n",
      "   ----------- ---------------------------- 0.4/1.3 MB 73.5 kB/s eta 0:00:13\n",
      "   ----------- ---------------------------- 0.4/1.3 MB 73.5 kB/s eta 0:00:13\n",
      "   ------------ --------------------------- 0.4/1.3 MB 76.5 kB/s eta 0:00:13\n",
      "   ------------- -------------------------- 0.4/1.3 MB 81.2 kB/s eta 0:00:12\n",
      "   ------------- -------------------------- 0.4/1.3 MB 81.2 kB/s eta 0:00:12\n",
      "   ------------- -------------------------- 0.5/1.3 MB 82.4 kB/s eta 0:00:11\n",
      "   -------------- ------------------------- 0.5/1.3 MB 85.2 kB/s eta 0:00:11\n",
      "   -------------- ------------------------- 0.5/1.3 MB 85.2 kB/s eta 0:00:11\n",
      "   -------------- ------------------------- 0.5/1.3 MB 88.3 kB/s eta 0:00:10\n",
      "   --------------- ------------------------ 0.5/1.3 MB 89.4 kB/s eta 0:00:10\n",
      "   --------------- ------------------------ 0.5/1.3 MB 91.8 kB/s eta 0:00:09\n",
      "   --------------- ------------------------ 0.5/1.3 MB 91.8 kB/s eta 0:00:09\n",
      "   ---------------- ----------------------- 0.6/1.3 MB 96.0 kB/s eta 0:00:09\n",
      "   ---------------- ----------------------- 0.6/1.3 MB 96.0 kB/s eta 0:00:09\n",
      "   ----------------- ---------------------- 0.6/1.3 MB 98.5 kB/s eta 0:00:08\n",
      "   ----------------- ---------------------- 0.6/1.3 MB 99.7 kB/s eta 0:00:08\n",
      "   ----------------- ---------------------- 0.6/1.3 MB 100.4 kB/s eta 0:00:08\n",
      "   ------------------ --------------------- 0.6/1.3 MB 103.1 kB/s eta 0:00:08\n",
      "   ------------------ --------------------- 0.6/1.3 MB 104.0 kB/s eta 0:00:07\n",
      "   ------------------ --------------------- 0.6/1.3 MB 104.0 kB/s eta 0:00:07\n",
      "   ------------------- -------------------- 0.6/1.3 MB 106.1 kB/s eta 0:00:07\n",
      "   ------------------- -------------------- 0.7/1.3 MB 108.7 kB/s eta 0:00:07\n",
      "   ------------------- -------------------- 0.7/1.3 MB 108.7 kB/s eta 0:00:07\n",
      "   -------------------- ------------------- 0.7/1.3 MB 109.5 kB/s eta 0:00:07\n",
      "   -------------------- ------------------- 0.7/1.3 MB 111.7 kB/s eta 0:00:06\n",
      "   --------------------- ------------------ 0.7/1.3 MB 112.8 kB/s eta 0:00:06\n",
      "   --------------------- ------------------ 0.7/1.3 MB 115.0 kB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 0.7/1.3 MB 117.7 kB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 0.7/1.3 MB 117.7 kB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 0.8/1.3 MB 117.8 kB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 0.8/1.3 MB 120.5 kB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 0.8/1.3 MB 120.9 kB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 0.8/1.3 MB 120.9 kB/s eta 0:00:05\n",
      "   ------------------------ --------------- 0.8/1.3 MB 122.9 kB/s eta 0:00:05\n",
      "   ------------------------ --------------- 0.8/1.3 MB 124.8 kB/s eta 0:00:05\n",
      "   ------------------------- -------------- 0.8/1.3 MB 125.8 kB/s eta 0:00:04\n",
      "   ------------------------- -------------- 0.9/1.3 MB 128.0 kB/s eta 0:00:04\n",
      "   ------------------------- -------------- 0.9/1.3 MB 128.0 kB/s eta 0:00:04\n",
      "   -------------------------- ------------- 0.9/1.3 MB 128.6 kB/s eta 0:00:04\n",
      "   -------------------------- ------------- 0.9/1.3 MB 130.5 kB/s eta 0:00:04\n",
      "   --------------------------- ------------ 0.9/1.3 MB 132.6 kB/s eta 0:00:04\n",
      "   --------------------------- ------------ 0.9/1.3 MB 132.6 kB/s eta 0:00:04\n",
      "   --------------------------- ------------ 0.9/1.3 MB 132.9 kB/s eta 0:00:04\n",
      "   --------------------------- ------------ 0.9/1.3 MB 132.9 kB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 0.9/1.3 MB 134.6 kB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 1.0/1.3 MB 135.2 kB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 1.0/1.3 MB 137.2 kB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 1.0/1.3 MB 137.2 kB/s eta 0:00:03\n",
      "   ------------------------------ --------- 1.0/1.3 MB 139.7 kB/s eta 0:00:03\n",
      "   ------------------------------ --------- 1.0/1.3 MB 139.7 kB/s eta 0:00:03\n",
      "   ------------------------------ --------- 1.0/1.3 MB 141.3 kB/s eta 0:00:03\n",
      "   ------------------------------ --------- 1.0/1.3 MB 141.6 kB/s eta 0:00:03\n",
      "   ------------------------------- -------- 1.1/1.3 MB 143.7 kB/s eta 0:00:02\n",
      "   -------------------------------- ------- 1.1/1.3 MB 145.6 kB/s eta 0:00:02\n",
      "   -------------------------------- ------- 1.1/1.3 MB 145.6 kB/s eta 0:00:02\n",
      "   -------------------------------- ------- 1.1/1.3 MB 145.5 kB/s eta 0:00:02\n",
      "   --------------------------------- ------ 1.1/1.3 MB 147.6 kB/s eta 0:00:02\n",
      "   --------------------------------- ------ 1.1/1.3 MB 147.8 kB/s eta 0:00:02\n",
      "   --------------------------------- ------ 1.1/1.3 MB 149.6 kB/s eta 0:00:02\n",
      "   --------------------------------- ------ 1.1/1.3 MB 149.6 kB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 1.2/1.3 MB 151.0 kB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 1.2/1.3 MB 151.8 kB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 1.2/1.3 MB 153.2 kB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 1.2/1.3 MB 153.6 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 1.2/1.3 MB 155.0 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 1.2/1.3 MB 155.0 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 1.2/1.3 MB 156.4 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 1.2/1.3 MB 156.7 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 1.3/1.3 MB 158.4 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 1.3/1.3 MB 158.4 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.3/1.3 MB 158.4 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.3/1.3 MB 159.7 kB/s eta 0:00:01\n",
      "   ---------------------------------------  1.3/1.3 MB 161.3 kB/s eta 0:00:01\n",
      "   ---------------------------------------  1.3/1.3 MB 161.3 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.3/1.3 MB 162.2 kB/s eta 0:00:00\n",
      "Installing collected packages: lightgbm\n",
      "Successfully installed lightgbm-4.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca54bb70-d010-44c9-8d3f-da2c99157560",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import lightgbm\n",
    "from lightgbm import LGBMRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f468d7c-9cf1-430e-b12d-d5a3ef111b5f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000249 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 831\n",
      "[LightGBM] [Info] Number of data points in the train set: 197, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 543.483442\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    }
   ],
   "source": [
    "lgbm= LGBMRegressor().fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11834b52-e234-4438-ac5e-25d5612e576d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LGBMRegressor()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMRegressor</label><div class=\"sk-toggleable__content\"><pre>LGBMRegressor()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LGBMRegressor()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f2de5cf-750c-44e7-bac4-87ec85aa20a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred= lgbm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f52834b-3692-4ef6-87f3-81da977007c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "363.8712087611089"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(mean_squared_error(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ed61cd-7d75-4fe9-86eb-33e69914568e",
   "metadata": {},
   "source": [
    "Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "26c586fb-e538-43eb-a3b4-8f23b81044bd",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mType:\u001b[0m           LGBMRegressor\n",
       "\u001b[1;31mString form:\u001b[0m    LGBMRegressor()\n",
       "\u001b[1;31mFile:\u001b[0m           c:\\users\\ycanf\\anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py\n",
       "\u001b[1;31mDocstring:\u001b[0m      LightGBM regressor.\n",
       "\u001b[1;31mInit docstring:\u001b[0m\n",
       "Construct a gradient boosting model.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "boosting_type : str, optional (default='gbdt')\n",
       "    'gbdt', traditional Gradient Boosting Decision Tree.\n",
       "    'dart', Dropouts meet Multiple Additive Regression Trees.\n",
       "    'rf', Random Forest.\n",
       "num_leaves : int, optional (default=31)\n",
       "    Maximum tree leaves for base learners.\n",
       "max_depth : int, optional (default=-1)\n",
       "    Maximum tree depth for base learners, <=0 means no limit.\n",
       "learning_rate : float, optional (default=0.1)\n",
       "    Boosting learning rate.\n",
       "    You can use ``callbacks`` parameter of ``fit`` method to shrink/adapt learning rate\n",
       "    in training using ``reset_parameter`` callback.\n",
       "    Note, that this will ignore the ``learning_rate`` argument in training.\n",
       "n_estimators : int, optional (default=100)\n",
       "    Number of boosted trees to fit.\n",
       "subsample_for_bin : int, optional (default=200000)\n",
       "    Number of samples for constructing bins.\n",
       "objective : str, callable or None, optional (default=None)\n",
       "    Specify the learning task and the corresponding learning objective or\n",
       "    a custom objective function to be used (see note below).\n",
       "    Default: 'regression' for LGBMRegressor, 'binary' or 'multiclass' for LGBMClassifier, 'lambdarank' for LGBMRanker.\n",
       "class_weight : dict, 'balanced' or None, optional (default=None)\n",
       "    Weights associated with classes in the form ``{class_label: weight}``.\n",
       "    Use this parameter only for multi-class classification task;\n",
       "    for binary classification task you may use ``is_unbalance`` or ``scale_pos_weight`` parameters.\n",
       "    Note, that the usage of all these parameters will result in poor estimates of the individual class probabilities.\n",
       "    You may want to consider performing probability calibration\n",
       "    (https://scikit-learn.org/stable/modules/calibration.html) of your model.\n",
       "    The 'balanced' mode uses the values of y to automatically adjust weights\n",
       "    inversely proportional to class frequencies in the input data as ``n_samples / (n_classes * np.bincount(y))``.\n",
       "    If None, all classes are supposed to have weight one.\n",
       "    Note, that these weights will be multiplied with ``sample_weight`` (passed through the ``fit`` method)\n",
       "    if ``sample_weight`` is specified.\n",
       "min_split_gain : float, optional (default=0.)\n",
       "    Minimum loss reduction required to make a further partition on a leaf node of the tree.\n",
       "min_child_weight : float, optional (default=1e-3)\n",
       "    Minimum sum of instance weight (Hessian) needed in a child (leaf).\n",
       "min_child_samples : int, optional (default=20)\n",
       "    Minimum number of data needed in a child (leaf).\n",
       "subsample : float, optional (default=1.)\n",
       "    Subsample ratio of the training instance.\n",
       "subsample_freq : int, optional (default=0)\n",
       "    Frequency of subsample, <=0 means no enable.\n",
       "colsample_bytree : float, optional (default=1.)\n",
       "    Subsample ratio of columns when constructing each tree.\n",
       "reg_alpha : float, optional (default=0.)\n",
       "    L1 regularization term on weights.\n",
       "reg_lambda : float, optional (default=0.)\n",
       "    L2 regularization term on weights.\n",
       "random_state : int, RandomState object or None, optional (default=None)\n",
       "    Random number seed.\n",
       "    If int, this number is used to seed the C++ code.\n",
       "    If RandomState or Generator object (numpy), a random integer is picked based on its state to seed the C++ code.\n",
       "    If None, default seeds in C++ code are used.\n",
       "n_jobs : int or None, optional (default=None)\n",
       "    Number of parallel threads to use for training (can be changed at prediction time by\n",
       "    passing it as an extra keyword argument).\n",
       "\n",
       "    For better performance, it is recommended to set this to the number of physical cores\n",
       "    in the CPU.\n",
       "\n",
       "    Negative integers are interpreted as following joblib's formula (n_cpus + 1 + n_jobs), just like\n",
       "    scikit-learn (so e.g. -1 means using all threads). A value of zero corresponds the default number of\n",
       "    threads configured for OpenMP in the system. A value of ``None`` (the default) corresponds\n",
       "    to using the number of physical cores in the system (its correct detection requires\n",
       "    either the ``joblib`` or the ``psutil`` util libraries to be installed).\n",
       "\n",
       "    .. versionchanged:: 4.0.0\n",
       "\n",
       "importance_type : str, optional (default='split')\n",
       "    The type of feature importance to be filled into ``feature_importances_``.\n",
       "    If 'split', result contains numbers of times the feature is used in a model.\n",
       "    If 'gain', result contains total gains of splits which use the feature.\n",
       "**kwargs\n",
       "    Other parameters for the model.\n",
       "    Check http://lightgbm.readthedocs.io/en/latest/Parameters.html for more parameters.\n",
       "\n",
       "    .. warning::\n",
       "\n",
       "        \\*\\*kwargs is not supported in sklearn, it may cause unexpected issues.\n",
       "\n",
       "Note\n",
       "----\n",
       "A custom objective function can be provided for the ``objective`` parameter.\n",
       "In this case, it should have the signature\n",
       "``objective(y_true, y_pred) -> grad, hess``,\n",
       "``objective(y_true, y_pred, weight) -> grad, hess``\n",
       "or ``objective(y_true, y_pred, weight, group) -> grad, hess``:\n",
       "\n",
       "    y_true : numpy 1-D array of shape = [n_samples]\n",
       "        The target values.\n",
       "    y_pred : numpy 1-D array of shape = [n_samples] or numpy 2-D array of shape = [n_samples, n_classes] (for multi-class task)\n",
       "        The predicted values.\n",
       "        Predicted values are returned before any transformation,\n",
       "        e.g. they are raw margin instead of probability of positive class for binary task.\n",
       "    weight : numpy 1-D array of shape = [n_samples]\n",
       "        The weight of samples. Weights should be non-negative.\n",
       "    group : numpy 1-D array\n",
       "        Group/query data.\n",
       "        Only used in the learning-to-rank task.\n",
       "        sum(group) = n_samples.\n",
       "        For example, if you have a 100-document dataset with ``group = [10, 20, 40, 10, 10, 10]``, that means that you have 6 groups,\n",
       "        where the first 10 records are in the first group, records 11-30 are in the second group, records 31-70 are in the third group, etc.\n",
       "    grad : numpy 1-D array of shape = [n_samples] or numpy 2-D array of shape = [n_samples, n_classes] (for multi-class task)\n",
       "        The value of the first order derivative (gradient) of the loss\n",
       "        with respect to the elements of y_pred for each sample point.\n",
       "    hess : numpy 1-D array of shape = [n_samples] or numpy 2-D array of shape = [n_samples, n_classes] (for multi-class task)\n",
       "        The value of the second order derivative (Hessian) of the loss\n",
       "        with respect to the elements of y_pred for each sample point.\n",
       "\n",
       "For multi-class task, y_pred is a numpy 2-D array of shape = [n_samples, n_classes],\n",
       "and grad and hess should be returned in the same format."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c319c90d-25ef-4dd3-997e-b16125d810a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lgbm = LGBMRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c460ffb-2556-4e81-83db-dbcc9f2f46d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lgbm_params = {'learning_rate' : [0.01,0.1,0.5,1],\n",
    "              'n_estimators ' : [20,40,100,200,500,1000],\n",
    "              'max_depth' : [1,2,3,4,5,6,7,8,9,10]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "580b5568-2879-4803-a38a-e0f8a4770838",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mInit signature:\u001b[0m\n",
       "\u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mparam_grid\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;33m*\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mrefit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mpre_dispatch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'2*n_jobs'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0merror_score\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnan\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mreturn_train_score\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m     \n",
       "Exhaustive search over specified parameter values for an estimator.\n",
       "\n",
       "Important members are fit, predict.\n",
       "\n",
       "GridSearchCV implements a \"fit\" and a \"score\" method.\n",
       "It also implements \"score_samples\", \"predict\", \"predict_proba\",\n",
       "\"decision_function\", \"transform\" and \"inverse_transform\" if they are\n",
       "implemented in the estimator used.\n",
       "\n",
       "The parameters of the estimator used to apply these methods are optimized\n",
       "by cross-validated grid-search over a parameter grid.\n",
       "\n",
       "Read more in the :ref:`User Guide <grid_search>`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "estimator : estimator object\n",
       "    This is assumed to implement the scikit-learn estimator interface.\n",
       "    Either estimator needs to provide a ``score`` function,\n",
       "    or ``scoring`` must be passed.\n",
       "\n",
       "param_grid : dict or list of dictionaries\n",
       "    Dictionary with parameters names (`str`) as keys and lists of\n",
       "    parameter settings to try as values, or a list of such\n",
       "    dictionaries, in which case the grids spanned by each dictionary\n",
       "    in the list are explored. This enables searching over any sequence\n",
       "    of parameter settings.\n",
       "\n",
       "scoring : str, callable, list, tuple or dict, default=None\n",
       "    Strategy to evaluate the performance of the cross-validated model on\n",
       "    the test set.\n",
       "\n",
       "    If `scoring` represents a single score, one can use:\n",
       "\n",
       "    - a single string (see :ref:`scoring_parameter`);\n",
       "    - a callable (see :ref:`scoring`) that returns a single value.\n",
       "\n",
       "    If `scoring` represents multiple scores, one can use:\n",
       "\n",
       "    - a list or tuple of unique strings;\n",
       "    - a callable returning a dictionary where the keys are the metric\n",
       "      names and the values are the metric scores;\n",
       "    - a dictionary with metric names as keys and callables a values.\n",
       "\n",
       "    See :ref:`multimetric_grid_search` for an example.\n",
       "\n",
       "n_jobs : int, default=None\n",
       "    Number of jobs to run in parallel.\n",
       "    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
       "    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
       "    for more details.\n",
       "\n",
       "    .. versionchanged:: v0.20\n",
       "       `n_jobs` default changed from 1 to None\n",
       "\n",
       "refit : bool, str, or callable, default=True\n",
       "    Refit an estimator using the best found parameters on the whole\n",
       "    dataset.\n",
       "\n",
       "    For multiple metric evaluation, this needs to be a `str` denoting the\n",
       "    scorer that would be used to find the best parameters for refitting\n",
       "    the estimator at the end.\n",
       "\n",
       "    Where there are considerations other than maximum score in\n",
       "    choosing a best estimator, ``refit`` can be set to a function which\n",
       "    returns the selected ``best_index_`` given ``cv_results_``. In that\n",
       "    case, the ``best_estimator_`` and ``best_params_`` will be set\n",
       "    according to the returned ``best_index_`` while the ``best_score_``\n",
       "    attribute will not be available.\n",
       "\n",
       "    The refitted estimator is made available at the ``best_estimator_``\n",
       "    attribute and permits using ``predict`` directly on this\n",
       "    ``GridSearchCV`` instance.\n",
       "\n",
       "    Also for multiple metric evaluation, the attributes ``best_index_``,\n",
       "    ``best_score_`` and ``best_params_`` will only be available if\n",
       "    ``refit`` is set and all of them will be determined w.r.t this specific\n",
       "    scorer.\n",
       "\n",
       "    See ``scoring`` parameter to know more about multiple metric\n",
       "    evaluation.\n",
       "\n",
       "    See :ref:`sphx_glr_auto_examples_model_selection_plot_grid_search_digits.py`\n",
       "    to see how to design a custom selection strategy using a callable\n",
       "    via `refit`.\n",
       "\n",
       "    .. versionchanged:: 0.20\n",
       "        Support for callable added.\n",
       "\n",
       "cv : int, cross-validation generator or an iterable, default=None\n",
       "    Determines the cross-validation splitting strategy.\n",
       "    Possible inputs for cv are:\n",
       "\n",
       "    - None, to use the default 5-fold cross validation,\n",
       "    - integer, to specify the number of folds in a `(Stratified)KFold`,\n",
       "    - :term:`CV splitter`,\n",
       "    - An iterable yielding (train, test) splits as arrays of indices.\n",
       "\n",
       "    For integer/None inputs, if the estimator is a classifier and ``y`` is\n",
       "    either binary or multiclass, :class:`StratifiedKFold` is used. In all\n",
       "    other cases, :class:`KFold` is used. These splitters are instantiated\n",
       "    with `shuffle=False` so the splits will be the same across calls.\n",
       "\n",
       "    Refer :ref:`User Guide <cross_validation>` for the various\n",
       "    cross-validation strategies that can be used here.\n",
       "\n",
       "    .. versionchanged:: 0.22\n",
       "        ``cv`` default value if None changed from 3-fold to 5-fold.\n",
       "\n",
       "verbose : int\n",
       "    Controls the verbosity: the higher, the more messages.\n",
       "\n",
       "    - >1 : the computation time for each fold and parameter candidate is\n",
       "      displayed;\n",
       "    - >2 : the score is also displayed;\n",
       "    - >3 : the fold and candidate parameter indexes are also displayed\n",
       "      together with the starting time of the computation.\n",
       "\n",
       "pre_dispatch : int, or str, default='2*n_jobs'\n",
       "    Controls the number of jobs that get dispatched during parallel\n",
       "    execution. Reducing this number can be useful to avoid an\n",
       "    explosion of memory consumption when more jobs get dispatched\n",
       "    than CPUs can process. This parameter can be:\n",
       "\n",
       "        - None, in which case all the jobs are immediately\n",
       "          created and spawned. Use this for lightweight and\n",
       "          fast-running jobs, to avoid delays due to on-demand\n",
       "          spawning of the jobs\n",
       "\n",
       "        - An int, giving the exact number of total jobs that are\n",
       "          spawned\n",
       "\n",
       "        - A str, giving an expression as a function of n_jobs,\n",
       "          as in '2*n_jobs'\n",
       "\n",
       "error_score : 'raise' or numeric, default=np.nan\n",
       "    Value to assign to the score if an error occurs in estimator fitting.\n",
       "    If set to 'raise', the error is raised. If a numeric value is given,\n",
       "    FitFailedWarning is raised. This parameter does not affect the refit\n",
       "    step, which will always raise the error.\n",
       "\n",
       "return_train_score : bool, default=False\n",
       "    If ``False``, the ``cv_results_`` attribute will not include training\n",
       "    scores.\n",
       "    Computing training scores is used to get insights on how different\n",
       "    parameter settings impact the overfitting/underfitting trade-off.\n",
       "    However computing the scores on the training set can be computationally\n",
       "    expensive and is not strictly required to select the parameters that\n",
       "    yield the best generalization performance.\n",
       "\n",
       "    .. versionadded:: 0.19\n",
       "\n",
       "    .. versionchanged:: 0.21\n",
       "        Default value was changed from ``True`` to ``False``\n",
       "\n",
       "Attributes\n",
       "----------\n",
       "cv_results_ : dict of numpy (masked) ndarrays\n",
       "    A dict with keys as column headers and values as columns, that can be\n",
       "    imported into a pandas ``DataFrame``.\n",
       "\n",
       "    For instance the below given table\n",
       "\n",
       "    +------------+-----------+------------+-----------------+---+---------+\n",
       "    |param_kernel|param_gamma|param_degree|split0_test_score|...|rank_t...|\n",
       "    +============+===========+============+=================+===+=========+\n",
       "    |  'poly'    |     --    |      2     |       0.80      |...|    2    |\n",
       "    +------------+-----------+------------+-----------------+---+---------+\n",
       "    |  'poly'    |     --    |      3     |       0.70      |...|    4    |\n",
       "    +------------+-----------+------------+-----------------+---+---------+\n",
       "    |  'rbf'     |     0.1   |     --     |       0.80      |...|    3    |\n",
       "    +------------+-----------+------------+-----------------+---+---------+\n",
       "    |  'rbf'     |     0.2   |     --     |       0.93      |...|    1    |\n",
       "    +------------+-----------+------------+-----------------+---+---------+\n",
       "\n",
       "    will be represented by a ``cv_results_`` dict of::\n",
       "\n",
       "        {\n",
       "        'param_kernel': masked_array(data = ['poly', 'poly', 'rbf', 'rbf'],\n",
       "                                     mask = [False False False False]...)\n",
       "        'param_gamma': masked_array(data = [-- -- 0.1 0.2],\n",
       "                                    mask = [ True  True False False]...),\n",
       "        'param_degree': masked_array(data = [2.0 3.0 -- --],\n",
       "                                     mask = [False False  True  True]...),\n",
       "        'split0_test_score'  : [0.80, 0.70, 0.80, 0.93],\n",
       "        'split1_test_score'  : [0.82, 0.50, 0.70, 0.78],\n",
       "        'mean_test_score'    : [0.81, 0.60, 0.75, 0.85],\n",
       "        'std_test_score'     : [0.01, 0.10, 0.05, 0.08],\n",
       "        'rank_test_score'    : [2, 4, 3, 1],\n",
       "        'split0_train_score' : [0.80, 0.92, 0.70, 0.93],\n",
       "        'split1_train_score' : [0.82, 0.55, 0.70, 0.87],\n",
       "        'mean_train_score'   : [0.81, 0.74, 0.70, 0.90],\n",
       "        'std_train_score'    : [0.01, 0.19, 0.00, 0.03],\n",
       "        'mean_fit_time'      : [0.73, 0.63, 0.43, 0.49],\n",
       "        'std_fit_time'       : [0.01, 0.02, 0.01, 0.01],\n",
       "        'mean_score_time'    : [0.01, 0.06, 0.04, 0.04],\n",
       "        'std_score_time'     : [0.00, 0.00, 0.00, 0.01],\n",
       "        'params'             : [{'kernel': 'poly', 'degree': 2}, ...],\n",
       "        }\n",
       "\n",
       "    NOTE\n",
       "\n",
       "    The key ``'params'`` is used to store a list of parameter\n",
       "    settings dicts for all the parameter candidates.\n",
       "\n",
       "    The ``mean_fit_time``, ``std_fit_time``, ``mean_score_time`` and\n",
       "    ``std_score_time`` are all in seconds.\n",
       "\n",
       "    For multi-metric evaluation, the scores for all the scorers are\n",
       "    available in the ``cv_results_`` dict at the keys ending with that\n",
       "    scorer's name (``'_<scorer_name>'``) instead of ``'_score'`` shown\n",
       "    above. ('split0_test_precision', 'mean_train_precision' etc.)\n",
       "\n",
       "best_estimator_ : estimator\n",
       "    Estimator that was chosen by the search, i.e. estimator\n",
       "    which gave highest score (or smallest loss if specified)\n",
       "    on the left out data. Not available if ``refit=False``.\n",
       "\n",
       "    See ``refit`` parameter for more information on allowed values.\n",
       "\n",
       "best_score_ : float\n",
       "    Mean cross-validated score of the best_estimator\n",
       "\n",
       "    For multi-metric evaluation, this is present only if ``refit`` is\n",
       "    specified.\n",
       "\n",
       "    This attribute is not available if ``refit`` is a function.\n",
       "\n",
       "best_params_ : dict\n",
       "    Parameter setting that gave the best results on the hold out data.\n",
       "\n",
       "    For multi-metric evaluation, this is present only if ``refit`` is\n",
       "    specified.\n",
       "\n",
       "best_index_ : int\n",
       "    The index (of the ``cv_results_`` arrays) which corresponds to the best\n",
       "    candidate parameter setting.\n",
       "\n",
       "    The dict at ``search.cv_results_['params'][search.best_index_]`` gives\n",
       "    the parameter setting for the best model, that gives the highest\n",
       "    mean score (``search.best_score_``).\n",
       "\n",
       "    For multi-metric evaluation, this is present only if ``refit`` is\n",
       "    specified.\n",
       "\n",
       "scorer_ : function or a dict\n",
       "    Scorer function used on the held out data to choose the best\n",
       "    parameters for the model.\n",
       "\n",
       "    For multi-metric evaluation, this attribute holds the validated\n",
       "    ``scoring`` dict which maps the scorer key to the scorer callable.\n",
       "\n",
       "n_splits_ : int\n",
       "    The number of cross-validation splits (folds/iterations).\n",
       "\n",
       "refit_time_ : float\n",
       "    Seconds used for refitting the best model on the whole dataset.\n",
       "\n",
       "    This is present only if ``refit`` is not False.\n",
       "\n",
       "    .. versionadded:: 0.20\n",
       "\n",
       "multimetric_ : bool\n",
       "    Whether or not the scorers compute several metrics.\n",
       "\n",
       "classes_ : ndarray of shape (n_classes,)\n",
       "    The classes labels. This is present only if ``refit`` is specified and\n",
       "    the underlying estimator is a classifier.\n",
       "\n",
       "n_features_in_ : int\n",
       "    Number of features seen during :term:`fit`. Only defined if\n",
       "    `best_estimator_` is defined (see the documentation for the `refit`\n",
       "    parameter for more details) and that `best_estimator_` exposes\n",
       "    `n_features_in_` when fit.\n",
       "\n",
       "    .. versionadded:: 0.24\n",
       "\n",
       "feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
       "    Names of features seen during :term:`fit`. Only defined if\n",
       "    `best_estimator_` is defined (see the documentation for the `refit`\n",
       "    parameter for more details) and that `best_estimator_` exposes\n",
       "    `feature_names_in_` when fit.\n",
       "\n",
       "    .. versionadded:: 1.0\n",
       "\n",
       "See Also\n",
       "--------\n",
       "ParameterGrid : Generates all the combinations of a hyperparameter grid.\n",
       "train_test_split : Utility function to split the data into a development\n",
       "    set usable for fitting a GridSearchCV instance and an evaluation set\n",
       "    for its final evaluation.\n",
       "sklearn.metrics.make_scorer : Make a scorer from a performance metric or\n",
       "    loss function.\n",
       "\n",
       "Notes\n",
       "-----\n",
       "The parameters selected are those that maximize the score of the left out\n",
       "data, unless an explicit score is passed in which case it is used instead.\n",
       "\n",
       "If `n_jobs` was set to a value higher than one, the data is copied for each\n",
       "point in the grid (and not `n_jobs` times). This is done for efficiency\n",
       "reasons if individual jobs take very little time, but may raise errors if\n",
       "the dataset is large and not enough memory is available.  A workaround in\n",
       "this case is to set `pre_dispatch`. Then, the memory is copied only\n",
       "`pre_dispatch` many times. A reasonable value for `pre_dispatch` is `2 *\n",
       "n_jobs`.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> from sklearn import svm, datasets\n",
       ">>> from sklearn.model_selection import GridSearchCV\n",
       ">>> iris = datasets.load_iris()\n",
       ">>> parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
       ">>> svc = svm.SVC()\n",
       ">>> clf = GridSearchCV(svc, parameters)\n",
       ">>> clf.fit(iris.data, iris.target)\n",
       "GridSearchCV(estimator=SVC(),\n",
       "             param_grid={'C': [1, 10], 'kernel': ('linear', 'rbf')})\n",
       ">>> sorted(clf.cv_results_.keys())\n",
       "['mean_fit_time', 'mean_score_time', 'mean_test_score',...\n",
       " 'param_C', 'param_kernel', 'params',...\n",
       " 'rank_test_score', 'split0_test_score',...\n",
       " 'split2_test_score', ...\n",
       " 'std_fit_time', 'std_score_time', 'std_test_score']\n",
       "\u001b[1;31mFile:\u001b[0m           c:\\users\\ycanf\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\n",
       "\u001b[1;31mType:\u001b[0m           ABCMeta\n",
       "\u001b[1;31mSubclasses:\u001b[0m     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "34246913-7b6c-4b7e-aaa5-3e8f9d6d2873",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 240 candidates, totalling 2400 fits\n",
      "[LightGBM] [Warning] Unknown parameter: 20\n",
      "[LightGBM] [Warning] num_iterations is set=100, n_estimators= will be ignored. Current value: num_iterations=100\n",
      "[LightGBM] [Warning] Unknown parameter: 20\n",
      "[LightGBM] [Warning] num_iterations is set=100, n_estimators= will be ignored. Current value: num_iterations=100\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000207 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 831\n",
      "[LightGBM] [Info] Number of data points in the train set: 197, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 543.483442\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    }
   ],
   "source": [
    "lgbm_cv= GridSearchCV(lgbm,lgbm_params,cv=10,n_jobs=-1,verbose=3).fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a56dcbf0-eac8-406c-8b49-1de99d57f426",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.1, 'max_depth': 4, 'n_estimators ': 20}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbm_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8dd8d874-64dc-4d30-959e-9a0aac511ec6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000135 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 831\n",
      "[LightGBM] [Info] Number of data points in the train set: 197, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 543.483442\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    }
   ],
   "source": [
    "lgbm_tuned = LGBMRegressor(learning_rate= 0.1, max_depth=4, n_estimators=20).fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d013274d-14b1-42c3-9d7f-1be4d5627d16",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "371.34440690249164"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred=lgbm_tuned.predict(X_test)\n",
    "np.sqrt(mean_squared_error(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb303d7-bbc0-4fe5-bc08-1a94ed37ba91",
   "metadata": {},
   "source": [
    "# değişken önem düzeyi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "59f4b920-8457-4822-b423-4692d374c0f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Importance = pd.DataFrame({'Importance': lgbm_tuned.feature_importances_*100},\n",
    "                         index= X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "75ad370c-0d31-418f-9a39-b7ca14395e38",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnAAAAGwCAYAAAApE1iKAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABk2ElEQVR4nO3deVxUVf8H8M9lG4GBwQUdUAQREWURlSw3YBQFTZPSNDWVR9PcUHPJqEzNClvMLbUylUpLy60ywwXBVFxwwUyRXEDoEbJUZpSQ9fz+8PH+HAEFRfEOn/frdV4Pc+65Z7l3bL7POXeRhBACRERERKQYZtXdASIiIiKqHAZwRERERArDAI6IiIhIYRjAERERESkMAzgiIiIihWEAR0RERKQwDOCIiIiIFMaiujtAVa+kpAQXL16EnZ0dJEmq7u4QERFRBQghcO3aNTg7O8PM7O5zbAzgTNDFixfh4uJS3d0gIiKi+5CZmYlGjRrdtQwDOBNkZ2cH4OYXwN7evpp7Q0RERBVhMBjg4uIi/47fDQM4E3Rr2dTe3p4BHBERkcJU5PIn3sRAREREpDAM4IiIiIgUhkuopkyjqe4eEBERmRYhqrsHADgDd9+ys7MRGRkJd3d3qFQquLi4oHfv3oiLiwMAuLm5QZIkSJIEa2treHl54cMPP4S47cSnp6fLZSRJgpWVFTw8PPDOO+8YlZs1axb8/f0f9RCJiIjoMcUZuPuQnp6Ojh07wsHBAR988AH8/PxQWFiIbdu2Ydy4cTh9+jQA4O2338bIkSNx48YN7Ny5E2PGjIG9vT1efvllo/p27twJb29v5OfnY+/evXjppZfg5OSEESNGVMfwiIiI6DHHAO4+jB07FpIk4dChQ7C1tZXzvb29MXz4cPmznZ0dtFotAOCll17CsmXLsH379lIBXN26deVyrq6uWLlyJY4ePcoAjoiIiMrEJdRKunLlCmJjYzFu3Dij4O0WBweHUnlCCCQkJCAlJQWWlpZ3rf/w4cM4evQonnzyyQr3KT8/HwaDwSgRERGR6WIAV0lnz56FEAJeXl73LDt9+nSo1WqoVCrodDoIITBhwoRS5Tp06AC1Wg0rKys88cQT6N+/P4YOHVrhPkVHR0Oj0ciJb2EgIiIybQzgKunWzQUVecjetGnTkJycjN27d0On0+GNN95Ahw4dSpVbt24dkpOTcfz4caxbtw4//PADXnvttQr3KSoqCnq9Xk6ZmZkVHxAREREpDq+Bq6RmzZpBkiSkpKQgPDz8rmXr1asHDw8PeHh4YMOGDfDw8MBTTz2FkJAQo3IuLi7w8PAAALRo0QLnz5/HjBkzMGvWLNSqVeuefVKpVFCpVPc9JiIiIlIWzsBVUp06dRAaGoolS5YgNze31PacnJwy96tduzYiIyMxdepUo0eElMXc3BxFRUUoKCioii4TERGRiWEAdx+WLl2K4uJitGvXDhs2bMCZM2eQkpKCRYsWoX379uXuN27cOKSmpmLDhg1G+ZcvX0Z2djb+/PNP/PLLL1i4cCF0Oh3fY0pERERl4hLqfWjSpAmOHj2Kd999F1OmTEFWVhYcHR3Rtm1bLFu2rNz9HB0dMWTIEMyaNQvPPfecnH9rSdXc3BxOTk7o2bMn3n333QfvqF4PMAgkIiIyOZK413oeKY7BYIBGo4Fer+csHhERkUJU5vebS6hERERECsMAjoiIiEhhGMARERERKQwDOCIiIiKFYQBHREREpDAM4IiIiIgUhgEcERERkcLwQb6mTKOp7h4QERH9Pz56tspwBo6IiIhIYUw6gIuIiIAkSZAkCZaWlnB3d8fUqVPLfAl9WYKDgzFp0qQyt3355Zdo164dbG1tYWdnh8DAQGzZsuW++hgeHl7p/YiIiKjmMukADgDCwsKQlZWF8+fP45133sHSpUsxderUB6pz6tSpePnll9G/f38cP34chw4dQufOndGnTx988sknVdRzIiIionIIEzZs2DDRp08fo7yXXnpJaLXaMrdNnDhRBAUFyfsCMEppaWli//79AoBYtGhRqfYmT54sLC0tRUZGhhBCiJkzZ4pWrVoZlZk/f75wdXWVt9/ZRnx8vMjPzxfjxo0TWq1WqFQq4erqKt57771yx3njxg2h1+vllJmZKQAI/c2rDZiYmJiYmB6PRHel1+sFAKHX6+9Z1uRn4O5kbW2NwsLCe5ZbuHAh2rdvj5EjRyIrKwtZWVlwcXHBt99+C7VajZdffrnUPlOmTEFhYSE2bNhQob5MnToV/fv3l2cJs7Ky0KFDByxatAg//vgjvvvuO6SmpmL16tVwc3Mrt57o6GhoNBo5ubi4VKh9IiIiUqYadRfqoUOH8M0336Br1673LKvRaGBlZQUbGxtotVo5/48//kDTpk1hZWVVah9nZ2doNBr88ccfFeqPWq2GtbU18vPzjdrIyMhAs2bN0KlTJ0iSBFdX17vWExUVhcmTJ8ufDQYDgzgiIiITZvIzcFu2bIFarUatWrXQvn17BAYGYvHixQ+tPSEEJEl6oDoiIiKQnJyM5s2bY8KECdi+fftdy6tUKtjb2xslIiIiMl0mH8DpdDokJycjNTUVN27cwMaNG1G/fn2YmZlBCGFUtiJLq56enjh37hwKCgpKbbt48SIMBgOaNWsGAPfdRps2bZCWloY5c+YgLy8P/fv3R79+/e65HxEREdUMJh/A2drawsPDA66urrC0tJTzHR0dkZWVZVQ2OTnZ6LOVlRWKi4uN8l544QVcv34dn332Wam2PvroI1haWqJv375yG9nZ2UZBXEXaAAB7e3sMGDAAy5cvx7p167BhwwZcuXKlQmMmIiIi01ajroG7XZcuXfDhhx/iq6++Qvv27bF69Wr8/vvvaN26tVzGzc0NBw8eRHp6OtRqNerUqYP27dtj4sSJmDZtGgoKChAeHo7CwkKsXr0aCxcuxIIFC+Trz4KDg/H333/jgw8+QL9+/RAbG4tffvnFaInTzc0N27ZtQ2pqKurWrQuNRoNPPvkETk5O8Pf3h5mZGb7//ntotVo4ODhUbpB6PcDlVCIiIpNj8jNw5QkNDcWMGTPw6quv4oknnsC1a9cwdOhQozJTp06Fubk5WrZsCUdHR2RkZAAAFixYgKVLl2Lt2rXw9fVF27ZtsXv3bmzevBmRkZHy/i1atMDSpUuxZMkStGrVCocOHSr1DLqRI0eiefPmCAgIgKOjI/bt2we1Wo33338fAQEBeOKJJ5Ceno6tW7fCzKzGni4iIiK6jSTuvEiLFM9gMECj0UCv1/OGBiIiIoWozO83p3SIiIiIFIYBHBEREZHCMIAjIiIiUhgGcEREREQKwwCOiIiISGEYwBEREREpDAM4IiIiIoWpsW9iqBE0muruARFR9eKjTslEcQbuHoQQCAkJQWhoaKltS5cuhUajkd/QQERERPQoMIC7B0mSsGrVKhw8eNDoBfZpaWmYPn06Fi5ciMaNG1dpm4WFhVVaHxEREZkWBnAV4OLigoULF2Lq1KlIS0uDEAIjRoxA165d0a5dO/Ts2RNqtRoNGjTAkCFD8M8//8j7xsbGolOnTnBwcEDdunXRq1cvnDt3Tt6enp4OSZLw3XffITg4GLVq1cLq1atx4cIF9O7dG7Vr14atrS28vb2xdevW6hg+ERERPWb4LtRKCA8PR05ODvr27Ys5c+YgKSkJAQEBGDlyJIYOHYq8vDxMnz4dRUVF2LVrFwBgw4YNkCQJvr6+yM3NxVtvvYX09HQkJyfDzMwM6enpaNKkCdzc3DBv3jy0bt0aKpUKo0aNQkFBAebNmwdbW1ucOnUK9vb2CAwMLNWv/Px85Ofny58NBgNcXFygB8A3oRJRjcafOFKQyrwLlQFcJVy6dAk+Pj64fPky1q9fj2PHjuHgwYPYtm2bXObPP/+Ei4sLUlNT4enpWaqOv//+G/Xr18eJEyfg4+MjB3ALFizAxIkT5XJ+fn7o27cvZs6cec9+zZo1C7Nnzy6VzwCOiGo8/sSRgvBl9g9J/fr1MWrUKLRo0QLPPvssjhw5gvj4eKjVajl5eXkBgLxMeu7cOQwaNAju7u6wt7dHkyZNAKDUjQ8BAQFGnydMmIB33nkHHTt2xMyZM/Hbb7+V26+oqCjo9Xo5ZWZmVuWwiYiI6DHDAK6SLCwsYGFx8+krJSUl6N27N5KTk43SmTNn5KXO3r174/Lly1i+fDkOHjyIgwcPAgAKCgqM6rW1tTX6/NJLL+H8+fMYMmQITpw4gYCAACxevLjMPqlUKtjb2xslIiIiMl0M4B5AmzZtcPLkSbi5ucHDw8Mo2dra4vLly0hJScGbb76Jrl27okWLFrh69WqF63dxccHo0aOxceNGTJkyBcuXL3+IoyEiIiKlYAD3AMaNG4crV65g4MCBOHToEM6fP4/t27dj+PDhKC4uRu3atVG3bl18/vnnOHv2LHbt2oXJkydXqO5JkyZh27ZtSEtLw9GjR7Fr1y60aNHiIY+IiIiIlIBvYngAzs7O2LdvH6ZPn47Q0FDk5+fD1dUVYWFhMDMzgyRJWLt2LSZMmAAfHx80b94cixYtQnBw8D3rLi4uxrhx4/Dnn3/C3t4eYWFhmD9/fuU6qNcDXE4lIiIyObwL1QRV5i4WIiIiejzwLlQiIiIiE8YAjoiIiEhhGMARERERKQwDOCIiIiKFYQBHREREpDAM4IiIiIgUhgEcERERkcLwQb6mTKOp7h4QkZLxMaFEjy3OwBEREREpTI0O4LKzsxEZGQl3d3eoVCq4uLigd+/eiIuLMyr33nvvwdzcHHPnzi1VR3BwMCZNmmSUl56eDkmS5GRlZQUPDw+88847qOyLLyRJwubNmys7NCIiIjJhNXYJNT09HR07doSDgwM++OAD+Pn5obCwENu2bcO4ceNw+vRpueyqVavw6quvYuXKlXjttdcq3MbOnTvh7e2N/Px87N27Fy+99BKcnJwwYsSIhzEkIiIiqilEDdWjRw/RsGFDcf369VLbrl69Kv+dkJAgGjZsKAoKCoSzs7PYvXu3vG3YsGECgFFKS0sTaWlpAoA4duyYUb1dunQRY8eOlT8fOnRIhISEiLp16wp7e3sRGBgojhw5Im93dXU1qtvV1bXMsdy4cUPo9Xo5ZWZmCgBCf/MKFiYmJqb7S0T0SOn1egFA6PX6e5atkUuoV65cQWxsLMaNGwdbW9tS2x0cHOS/V6xYgYEDB8LS0hIDBw7EihUr5G0LFy5E+/btMXLkSGRlZSErKwsuLi5ltnn48GEcPXoUTz75pJx37do1DBs2DHv27MGBAwfQrFkz9OzZE9euXQMAJCUlAbg5A5iVlSV/vlN0dDQ0Go2cyusDERERmYYauYR69uxZCCHg5eV113IGgwEbNmxAYmIiAODFF19Ex44dsXjxYtjb20Oj0cDKygo2NjbQarWl9u/QoQPMzMxQUFCAwsJCjBo1CkOHDpW3d+nSxaj8Z599htq1a2P37t3o1asXHB0dAdwMKMuq/5aoqChMnjzZqN8M4oiIiExXjZyBE0IAuHmDwN188803cHd3R6tWrQAA/v7+cHd3x9q1ayvUzrp165CcnIzjx49j3bp1+OGHH4yuobt06RJGjx4NT09Pefbs+vXryMjIqNR4VCoV7O3tjRIRERGZrhoZwDVr1gySJCElJeWu5VauXImTJ0/CwsJCTidPnjRaRr0bFxcXeHh4oEWLFujfvz8mTZqEefPm4caNGwCAiIgIHDlyBAsWLEBiYiKSk5NRt25dFBQUPPAYiYiIyHTVyCXUOnXqIDQ0FEuWLMGECRNKXQeXk5ODzMxMHD58GAkJCahTp47RtsDAQPz+++/w8fGBlZUViouLK9Suubk5ioqKUFBQgFq1amHPnj1YunQpevbsCQDIzMzEP//8Y7SPpaVlhesnIiKimqFGBnAAsHTpUnTo0AHt2rXD22+/DT8/PxQVFWHHjh1YtmwZQkND0a5dOwQGBpbat3379lixYgXmz58PNzc3HDx4EOnp6VCr1UbB3uXLl5GdnY2ioiKcOHECCxcuhE6nk5c4PTw88PXXXyMgIAAGgwHTpk2DtbW1UVtubm6Ii4tDx44doVKpULt27YoPUq8HuJxKRERkcmrkEioANGnSBEePHoVOp8OUKVPg4+ODbt26IS4uDgsXLsTq1avRt2/fMvft27cvVq9ejYKCAkydOhXm5uZo2bIlHB0dja5fCwkJgZOTE9zc3DBq1Cj07NkT69atk7evXLkSV69eRevWrTFkyBBMmDAB9evXN2pr3rx52LFjB1xcXNC6deuHczCIiIhIUSRx64p+MhkGgwEajQZ6vZ43NBARESlEZX6/a+wMHBEREZFSMYAjIiIiUhgGcEREREQKwwCOiIiISGEYwBEREREpDAM4IiIiIoVhAEdERESkMAzgHoGEhARIkoScnBwAQExMDBwcHB5+wxoNIElMTExM95eI6LHFAO4On376Kezs7FBUVCTnXb9+HZaWlujcubNR2T179kCSJPzxxx+PuptERERUgzGAu4NOp8P169dx+PBhOW/Pnj3QarVISkrCv//+K+cnJCTA2dkZnp6e1dFVIiIiqqEYwN2hefPmcHZ2RkJCgpyXkJCAPn36oGnTpkhMTDTK1+l0WL16NQICAmBnZwetVotBgwbh0qVLFW7z8uXLaNeuHZ555hncuHEDV69exeDBg+Ho6Ahra2s0a9YMq1atqsphEhERkYIxgCtDcHAw4uPj5c/x8fEIDg5GUFCQnF9QUID9+/dDp9OhoKAAc+bMwfHjx7F582akpaUhIiKiQm39+eef6Ny5M7y8vLBx40bUqlULM2bMwKlTp/DLL78gJSUFy5YtQ7169cqtIz8/HwaDwSgRERGR6bKo7g48joKDg/HKK6+gqKgIeXl5OHbsGAIDA1FcXIxFixYBAA4cOIC8vDzodDq4u7vL+7q7u2PRokVo164drl+/DrVaXW47f/zxB7p164Y+ffpg4cKFkP530XBGRgZat26NgIAAAICbm9td+xsdHY3Zs2c/4KiJiIhIKTgDVwadTofc3FwkJSVhz5498PT0RP369REUFISkpCTk5uYiISEBjRs3hru7O44dO4Y+ffrA1dUVdnZ2CA4OBnAzECtPXl4eOnXqhPDwcCxatEgO3gBgzJgxWLt2Lfz9/fHqq68aLduWJSoqCnq9Xk6ZmZlVchyIiIjo8cQArgweHh5o1KgR4uPjER8fj6CgIACAVqtFkyZNsG/fPsTHx6NLly7Izc1F9+7doVarsXr1aiQlJWHTpk0Abi6zlkelUiEkJAQ///wz/vzzT6NtPXr0wIULFzBp0iRcvHgRXbt2xdSpU+9al729vVEiIiIi08UArhw6nQ4JCQlISEiQZ9QAICgoCNu2bcOBAweg0+lw+vRp/PPPP5g7d658LVtFbmAwMzPD119/jbZt26JLly64ePGi0XZHR0dERERg9erVWLBgAT7//POqHiIREREpFAO4cuh0OuzduxfJycnyDBxwM4Bbvnw5bty4AZ1Oh8aNG8PKygqLFy/G+fPn8eOPP2LOnDkVasPc3Bxr1qxBq1at0KVLF2RnZwMA3nrrLfzwww84e/YsTp48iS1btqBFixYPZZxERESkPAzgyqHT6ZCXlwcPDw80aNBAzg8KCsK1a9fQtGlTuLi4wNHRETExMfj+++/RsmVLzJ07Fx999FGF27GwsMC3334Lb29vdOnSBZcuXYKVlRWioqLg5+eHwMBAmJubY+3atZUfhF4PCMHExMR0f4mIHluSEPxXamoMBgM0Gg30ej2vhyMiIlKIyvx+cwaOiIiISGEYwBEREREpDAM4IiIiIoVhAEdERESkMAzgiIiIiBSGARwRERGRwjCAIyIiIlIYi+ruQE0TERGBnJwcbN68+eE3ptE8/DaIyHTxMaFEjy3OwFVQdnY2IiMj4e7uDpVKBRcXF/Tu3RtxcXEAADc3NyxYsKDUfrNmzYK/v7/8eeHChYiJiZE/BwcHY9KkSQ+380RERGRSOANXAenp6ejYsSMcHBzwwQcfwM/PD4WFhdi2bRvGjRuH06dPV7guDWfFiIiI6AFxBq4Cxo4dC0mScOjQIfTr1w+enp7w9vbG5MmTceDAgUrVFRERgfDwcPnv3bt3Y+HChZAkCZIkIT09HVevXsXgwYPh6OgIa2trNGvWDKtWrXoIIyMiIiIl4gzcPVy5cgWxsbF49913YWtrW2q7g4PDfde9cOFC/PHHH/Dx8cHbb78NAHB0dMTEiRNx6tQp/PLLL6hXrx7Onj2LvLy8cuvJz89Hfn6+/NlgMNx3n4iIiOjxxwDuHs6ePQshBLy8vO5Zdvr06XjzzTeN8goKCtCyZcsyy2s0GlhZWcHGxgZarVbOz8jIQOvWrREQEADg5vV1dxMdHY3Zs2ffs39ERERkGriEeg/if3dhSZJ0z7LTpk1DcnKyURo9enSl2xwzZgzWrl0Lf39/vPrqq0hMTLxr+aioKOj1ejllZmZWuk0iIiJSDgZw99CsWTNIkoSUlJR7lq1Xrx48PDyMUp06dSrdZo8ePXDhwgVMmjQJFy9eRNeuXTF16tRyy6tUKtjb2xslIiIiMl0M4O6hTp06CA0NxZIlS5Cbm1tqe05OzgPVb2VlheLi4lL5jo6OiIiIwOrVq7FgwQJ8/vnnD9QOERERmQ4GcBWwdOlSFBcXo127dtiwYQPOnDmDlJQULFq0CO3bt3+gut3c3HDw4EGkp6fjn3/+QUlJCd566y388MMPOHv2LE6ePIktW7agRYsWVTQaIiIiUjrexFABTZo0wdGjR/Huu+9iypQpyMrKgqOjI9q2bYtly5Y9UN1Tp07FsGHD0LJlS+Tl5SEtLQ1WVlaIiopCeno6rK2t0blzZ6xdu7bylev1AJdTiYiITI4kBN+VYmoMBgM0Gg30ej2vhyMiIlKIyvx+cwmViIiISGEYwBEREREpDAM4IiIiIoVhAEdERESkMAzgiIiIiBSGARwRERGRwjCAIyIiIlIYBnBERERECsM3MZgyjaa6e0BEDwufwU5Uo3EG7g7Z2dmIjIyEu7s7VCoVXFxc0Lt3b8TFxQG4+e5SSZIgSRKsra3h5eWFDz/8EHyhBRERET0qnIG7TXp6Ojp27AgHBwd88MEH8PPzQ2FhIbZt24Zx48bh9OnTAIC3334bI0eOxI0bN7Bz506MGTMG9vb2ePnll6t5BERERFQTcAbuNmPHjoUkSTh06BD69esHT09PeHt7Y/LkyThw4IBczs7ODlqtFm5ubnjppZfg5+eH7du3y9slScLmzZuN6nZwcEBMTAyAm4GiJEnYuHEjdDodbGxs0KpVK+zfv18uf+HCBfTu3Ru1a9eGra0tvL29sXXr1oc6fiIiIlIGBnD/c+XKFcTGxmLcuHGwtbUttd3BwaFUnhACCQkJSElJgaWlZaXbfOONNzB16lQkJyfD09MTAwcORFFREQBg3LhxyM/Px6+//ooTJ07g/fffh1qtLrOe/Px8GAwGo0RERESmiwHc/5w9exZCCHh5ed2z7PTp06FWq6FSqaDT6SCEwIQJEyrd5tSpU/H000/D09MTs2fPxoULF3D27FkAQEZGBjp27AhfX1+4u7ujV69eCAwMLLOe6OhoaDQaObm4uFS6L0RERKQcDOD+59ZNCJIk3bPstGnTkJycjN27d0On0+GNN95Ahw4dKt2mn5+f/LeTkxMA4NKlSwCACRMm4J133kHHjh0xc+ZM/Pbbb+XWExUVBb1eL6fMzMxK94WIiIiUgwHc/zRr1gySJCElJeWeZevVqwcPDw+0b98eGzZswPz587Fz5055uyRJpe5KLSwsLFXP7cuutwLHkpISAMBLL72E8+fPY8iQIThx4gQCAgKwePHiMvujUqlgb29vlIiIiMh0MYD7nzp16iA0NBRLlixBbm5uqe05OTll7le7dm1ERkZi6tSpctDm6OiIrKwsucyZM2fw77//VrpPLi4uGD16NDZu3IgpU6Zg+fLlla6DiIiITA8DuNssXboUxcXFaNeuHTZs2IAzZ84gJSUFixYtQvv27cvdb9y4cUhNTcWGDRsAAF26dMEnn3yCo0eP4vDhwxg9enSlb3KYNGkStm3bhrS0NBw9ehS7du1CixYtKjcgvf7mwz6ZmJhMLxFRjcYA7jZNmjTB0aNHodPpMGXKFPj4+KBbt26Ii4vDsmXLyt3P0dERQ4YMwaxZs1BSUoJ58+bBxcUFgYGBGDRoEKZOnQobG5tK9aW4uBjjxo1DixYtEBYWhubNm2Pp0qUPOkQiIiIyAZLgKwRMjsFggEajgV6v5/VwREREClGZ32/OwBEREREpDAM4IiIiIoVhAEdERESkMAzgiIiIiBSGARwRERGRwjCAIyIiIlIYBnBERERECmNR3R2oKWbNmoXNmzcjOTkZABAREYGcnBxs3rz54TWq0Ty8uomofHy8JhE9ZDVyBi47OxuRkZFwd3eHSqWCi4sLevfujbi4OLzwwgvo0aOHUflffvkFkiRhxowZRvlz5syBs7Pzo+w6ERERUc0L4NLT09G2bVvs2rULH3zwAU6cOIHY2FjodDqMGzcOOp0Oe/fuRVFRkbxPQkICXFxcEB8fb1RXQkICdDrdox4CERER1XA1LoAbO3YsJEnCoUOH0K9fP3h6esLb2xuTJ0/GgQMHoNPpcP36dRw+fFjeJyEhAa+99hqSkpLw77//AgAKCgqwf/9+OYCbPn06PD09YWNjA3d3d8yYMQOFhYUV7teRI0dQv359vPvuuwCA48ePQ6fTwc7ODvb29mjbtq1Rn4iIiKjmqlEB3JUrVxAbG4tx48bB1ta21HYHBwd4enrC2dlZnm27du0ajh49iueffx5NmzbFvn37AAAHDhxAXl6eHMDZ2dkhJiYGp06dwsKFC7F8+XLMnz+/Qv1KSEhA165dMXv2bLzxxhsAgMGDB6NRo0ZISkrCkSNH8Nprr8HS0rLM/fPz82EwGIwSERERma4aFcCdPXsWQgh4eXndtVxwcDASEhIAAHv27IGnpyccHR0RFBQk599aVm3atCkA4M0330SHDh3g5uaG3r17Y8qUKfjuu+/u2acffvgBzzzzDJYtW4YxY8bI+RkZGQgJCYGXlxeaNWuG559/Hq1atSqzjujoaGg0Gjm5uLhU4GgQERGRUtWoAE78784wSZLuWk6n02Hfvn0oLCxEQkICgoODAaBUANelSxd5n/Xr16NTp07QarVQq9WYMWMGMjIy7trOwYMH0bdvX3z55ZcYOHCg0bbJkyfjpZdeQkhICObOnYtz586VW09UVBT0er2cMjMz79ouERERKVuNCuCaNWsGSZKQkpJy13I6nQ65ublISkpCfHw8goKCANwM4JKSknDlyhWj698OHDgg3726ZcsWHDt2DG+88QYKCgru2k7Tpk3h5eWFlStXlio7a9YsnDx5Ek8//TR27dqFli1bYtOmTWXWo1KpYG9vb5SIiIjIdNWoAK5OnToIDQ3FkiVLkJubW2p7Tk4OgJuBlYuLC3788UckJyfLAZyTkxPc3Nwwb9483LhxQw7g9u3bB1dXV7zxxhsICAhAs2bNcOHChXv2p169eti1axfOnTuHAQMGlLrpwdPTE6+88gq2b9+O5557DqtWrXrAI0BERESmoEYFcACwdOlSFBcXo127dtiwYQPOnDmDlJQULFq0CO3bt5fL6XQ6LF26FB4eHmjQoIGcHxQUhMWLF8Pd3R2NGzcGAHh4eCAjIwNr167FuXPnsGjRonJny+5Uv3597Nq1C6dPn8bAgQNRVFSEvLw8jB8/HgkJCbhw4QL27duHpKQktGjRomoPBhERESlSjQvgmjRpgqNHj0Kn02HKlCnw8fFBt27dEBcXh2XLlsnldDodrl27Jl//dktQUBCuXbtm9Py3Pn364JVXXsH48ePh7++PxMTEUg/9vRutVotdu3bhxIkTGDx4MMzMzHD58mUMHToUnp6e6N+/P3r06IHZs2dXbrB6/c0nwjMxMT3aRET0kElC8L82psZgMECj0UCv1/N6OCIiIoWozO93jZuBIyIiIlI6BnBERERECsMAjoiIiEhhGMARERERKQwDOCIiIiKFYQBHREREpDAM4IiIiIgUhgEcERERkcJYVHcHapqYmBhMmjRJfu/qQ6XRPPw2iKoSnytORFQhnIGrQhEREQgPDy+Vn5CQAEmSkJOTgwEDBuCPP/6Qt82aNQv+/v6PrpNERESkeJyBe8Ssra1hbW1d3d0gIiIiBeMM3CMWExMDBwcH+e/Zs2fj+PHjkCQJkiQhJiYGwM2ZucaNG0OlUsHZ2RkTJkwot878/HwYDAajRERERKaLM3DVaMCAAfj9998RGxuLnTt3AgA0Gg3Wr1+P+fPnY+3atfD29kZ2djaOHz9ebj3R0dGYPXv2o+o2ERERVTMGcFVsy5YtUKvVRnnFxcVllrW2toZarYaFhQW0Wq2cn5GRAa1Wi5CQEFhaWqJx48Zo165duW1GRUVh8uTJ8meDwQAXF5cHHAkRERE9rriEWsV0Oh2Sk5ON0hdffFGpOp5//nnk5eXB3d0dI0eOxKZNm1BUVFRueZVKBXt7e6NEREREposBXBWztbWFh4eHUWrYsGGl6nBxcUFqaiqWLFkCa2trjB07FoGBgSgsLHxIvSYiIiIlYQBXzaysrMpcYrW2tsYzzzyDRYsWISEhAfv378eJEyeqoYdERET0uOE1cNXMzc0NaWlpSE5ORqNGjWBnZ4dvv/0WxcXFePLJJ2FjY4Ovv/4a1tbWcHV1rVzlej3A5VQiIiKTwxm4ata3b1+EhYVBp9PB0dER3377LRwcHLB8+XJ07NgRfn5+iIuLw08//YS6detWd3eJiIjoMSAJwXfXmBqDwQCNRgO9Xs8bGoiIiBSiMr/fnIEjIiIiUhgGcEREREQKwwCOiIiISGEYwBEREREpDAM4IiIiIoVhAEdERESkMAzgiIiIiBSGb2IwZRpNdfeATAkfGUlE9NjgDFwFJCYmwtzcHGFhYUb5s2bNgr+/f6nybm5ukCQJkiTB3Nwczs7OGDFiBK5evVqpdoODgzFp0qQH6DkRERGZIgZwFbBy5UpERkZi7969yMjIqNA+b7/9NrKyspCRkYE1a9bg119/xYQJEx5yT4mIiKgmYAB3D7m5ufjuu+8wZswY9OrVCzExMQCAmJgYzJ49G8ePH5dn225tAwA7OztotVo0bNgQOp0OQ4cOxdGjR+Xtly9fxsCBA9GoUSPY2NjA19cX3377rbw9IiICu3fvxsKFC+X609PTH9GoiYiI6HHGAO4e1q1bh+bNm6N58+Z48cUXsWrVKgghMGDAAEyZMgXe3t7IyspCVlYWBgwYUGYd//3vf7FlyxY8+eSTct6NGzfQtm1bbNmyBb///jtGjRqFIUOG4ODBgwCAhQsXon379hg5cqRcv4uLS5n15+fnw2AwGCUiIiIyXQzg7mHFihV48cUXAQBhYWG4fv064uLiYG1tDbVaDQsLC2i1Wmi1WlhbW8v7TZ8+HWq1GtbW1mjUqBEkScLHH38sb2/YsCGmTp0Kf39/uLu7IzIyEqGhofj+++8BABqNBlZWVrCxsZHrNzc3L7OP0dHR0Gg0ciov0CMiIiLTwADuLlJTU3Ho0CG88MILAAALCwsMGDAAK1euvOe+06ZNQ3JyMn777TfExcUBAJ5++mkUFxcDAIqLi/Huu+/Cz88PdevWhVqtxvbt2yt8jd3toqKioNfr5ZSZmVnpOoiIiEg5+BiRu1ixYgWKiorQsGFDOU8IAUtLy3veUVqvXj14eHgAAJo1a4YFCxagffv2iI+PR0hICObNm4f58+djwYIF8PX1ha2tLSZNmoSCgoJK91OlUkGlUlV6PyIiIlImBnDlKCoqwldffYV58+ahe/fuRtv69u2LNWvWwMrKSp5Ru5dby595eXkAgD179qBPnz7y8mxJSQnOnDmDFi1ayPtUpn4iIiKqORjAlWPLli24evUqRowYAc0dD8Tt168fVqxYgWnTpiEtLQ3Jyclo1KgR7Ozs5Jmwa9euITs7G0IIZGZm4tVXX0W9evXQoUMHAICHhwc2bNiAxMRE1K5dGx9//DGys7ONAjg3NzccPHgQ6enpUKvVqFOnDszMuOpNRERU0zEaKMeKFSsQEhJSKngDbs7AJScno2nTpggLC4NOp4Ojo6PRY0DeeustODk5wdnZGb169YKtrS127NiBunXrAgBmzJiBNm3aIDQ0FMHBwdBqtQgPDzdqZ+rUqTA3N0fLli3h6OhY+evj9PqbT89nYqqKREREjw1JCP6X2dQYDAZoNBro9XrY29tXd3eIiIioAirz+80ZOCIiIiKFYQBHREREpDAM4IiIiIgUhgEcERERkcIwgCMiIiJSGAZwRERERArDAI6IiIhIYRjAERERESkMX6Vlysp4iwSZKD6Pm4ioRuEM3EMSEREBSZIgSRIsLCzQuHFjjBkzBlevXpXLuLm5yWXMzc3h7OyMESNGGJVJSEiAJEnIycmphlEQERHR44gB3EMUFhaGrKwspKen44svvsBPP/2EsWPHGpV5++23kZWVhYyMDKxZswa//vorJkyYUE09JiIiIiXgEupDpFKpoNVqAQCNGjXCgAEDEBMTY1TGzs5OLtOwYUMMHToUa9eurVQ7+fn5yM/Plz8bDIYH6zgRERE91jgD94icP38esbGxsLS0LLfMf//7X2zZsgVPPvlkpeqOjo6GRqORk4uLy4N2l4iIiB5jDOAeoi1btkCtVsPa2hpNmzbFqVOnMH36dKMy06dPl8s0atQIkiTh448/rlQ7UVFR0Ov1csrMzKzKYRAREdFjhgHcQ6TT6ZCcnIyDBw8iMjISoaGhiIyMNCozbdo0JCcn47fffkNcXBwA4Omnn0ZxcXGF21GpVLC3tzdKREREZLoYwD1Etra28PDwgJ+fHxYtWoT8/HzMnj3bqEy9evXg4eGBZs2aoUuXLliwYAESExMRHx9fTb0mIiKixx0DuEdo5syZ+Oijj3Dx4sVyy5ibmwMA8vLyHlW3iIiISGF4F+ojFBwcDG9vb7z33nv45JNPAADXrl1DdnY2hBDIzMzEq6++inr16qFDhw4P3qBeD3A5lYiIyORwBu4Rmzx5MpYvXy7faPDWW2/ByckJzs7O6NWrF2xtbbFjxw7UrVu3mntKREREjytJCL6Dx9QYDAZoNBro9Xre0EBERKQQlfn95gwcERERkcIwgCMiIiJSGAZwRERERArDAI6IiIhIYRjAERERESkMAzgiIiIihWEAR0RERKQwfBODKdNoqrsHdAsft0hERFXIpGbgsrOzERkZCXd3d6hUKri4uKB3796Ii4sDALi5uWHBggWl9ps1axb8/f0fuH1JkuSkVqvRqlUrxMTEPHC9RERERLczmRm49PR0dOzYEQ4ODvjggw/g5+eHwsJCbNu2DePGjcPp06cfST9WrVqFsLAw5ObmYt26dfjPf/4DJycnhIaGPpL2iYiIyPSZzAzc2LFjIUkSDh06hH79+sHT0xPe3t6YPHkyDhw4UKm6IiIiEB4ejvfeew8NGjSAg4MDZs+ejaKiIkybNg116tRBo0aNsHLlylL7Ojg4QKvVomnTpnj99ddRp04dbN++HcDNIFOSJCQnJ8vlc3JyIEkSEhISAAAJCQmQJAlxcXEICAiAjY0NOnTogNTU1Ps+NkRERGRaTCKAu3LlCmJjYzFu3DjY2tqW2u7g4FDpOnft2oWLFy/i119/xccff4xZs2ahV69eqF27Ng4ePIjRo0dj9OjR8kvp71RcXIzvvvsOV65cgaWlZaXbf+ONNzBv3jwcPnwYFhYWGD58eLll8/PzYTAYjBIRERGZMGECDh48KACIjRs33rWcq6ursLKyEra2tkbJ0tJStGrVSi43bNgw4erqKoqLi+W85s2bi86dO8ufi4qKhK2trfj222/lPACiVq1awtbWVpibmwsAok6dOuLMmTNCCCHS0tIEAHHs2DF5n6tXrwoAIj4+XgghRHx8vAAgdu7cKZf5+eefBQCRl5dX5rhmzpwpAJRK+puXzjM9DomIiOge9Hr9zd9vvf6eZU1iBk787w4/SZLuWXbatGlITk42SqNHjy5VztvbG2Zm/394GjRoAF9fX/mzubk56tati0uXLhntN3/+fCQnJ2PHjh3w9/fH/Pnz4eHhUekx+fn5yX87OTkBQKm2bomKioJer5dTebOCREREZBpM4iaGZs2aQZIkpKSkIDw8/K5l69WrVyqgqlOnTqlydy57SpJUZl5JSYlRnlarhYeHBzw8PPD999+jdevWCAgIQMuWLeWA8FbACQCFhYVl9vP2tm4Fpne2dYtKpYJKpSpzGxEREZkek5iBq1OnDkJDQ7FkyRLk5uaW2p6Tk/PoOwXAw8MDffv2RVRUFADA0dERAJCVlSWXuf2GBiIiIqKKMIkADgCWLl2K4uJitGvXDhs2bMCZM2eQkpKCRYsWoX379tXWrylTpuCnn37C4cOHYW1tjaeeegpz587FqVOn8Ouvv+LNN9+str4RERGRMplMANekSRMcPXoUOp0OU6ZMgY+PD7p164a4uDgsW7as2vrl6+uLkJAQvPXWWwCAlStXorCwEAEBAZg4cSLeeeedh9e4nrcxPDaJiIioCklC8NfF1BgMBmg0Guj1etjb21d3d4iIiKgCKvP7bTIzcEREREQ1BQM4IiIiIoVhAEdERESkMAzgiIiIiBSGARwRERGRwjCAIyIiIlIYBnBERERECmMS70Klcmg01d0DZeCjEImISGE4A0dERESkMAzgyhAREQFJkkqlsLCw6u4aEREREZdQyxMWFoZVq1YZ5alUqjLLFhYWwtLS8p55FXG/+xEREVHNwRm4cqhUKmi1WqNUu3ZtAIAkSfj000/Rp08f2Nra4p133sGsWbPg7++PlStXwt3dHSqVCkIIZGRkoE+fPlCr1bC3t0f//v3x119/ye2Ut9/69evh6+sLa2tr1K1bFyEhIcjNzS2zr/n5+TAYDEaJiIiITBcDuPs0c+ZM9OnTBydOnMDw4cMBAGfPnsV3332HDRs2IDk5GQAQHh6OK1euYPfu3dixYwfOnTuHAQMGGNV1537Z2dkYOHAghg8fjpSUFCQkJOC5556DKOdi++joaGg0Gjm5uLg81LETERFR9eISajm2bNkCtVptlDd9+nTMmDEDADBo0CA5cLuloKAAX3/9NRwdHQEAO3bswG+//Ya0tDQ5qPr666/h7e2NpKQkPPHEE2Xud/ToURQVFeG5556Dq6srAMDX17fcvkZFRWHy5MnyZ4PBwCCOiIjIhDGAK4dOp8OyZcuM8urUqSP/HRAQUGofV1dXOQgDgJSUFLi4uBgFUy1btoSDgwNSUlLkAO7O/Vq1aoWuXbvC19cXoaGh6N69O/r16ycv4d5JpVKVe30eERERmR4uoZbD1tYWHh4eRun2AM7W1rbMfW4nhIAkSaXK3Zl/537m5ubYsWMHfvnlF7Rs2RKLFy9G8+bNkZaW9qDDIiIiIhPAAO4hatmyJTIyMpCZmSnnnTp1Cnq9Hi1atLjrvpIkoWPHjpg9ezaOHTsGKysrbNq06WF3mYiIiBSAS6jlyM/PR3Z2tlGehYUF6tWrV+E6QkJC4Ofnh8GDB2PBggUoKirC2LFjERQUVOYS7C0HDx5EXFwcunfvjvr16+PgwYP4+++/7xn0laLXA/b2lduHiIiIHnsM4MoRGxsLJycno7zmzZvj9OnTFa5DkiRs3rwZkZGRCAwMhJmZGcLCwrB48eK77mdvb49ff/0VCxYsgMFggKurK+bNm4cePXrc11iIiIjItEiivGdTkGIZDAZoNBro9XrYcwaOiIhIESrz+81r4IiIiIgUhgEcERERkcIwgCMiIiJSGAZwRERERArDAI6IiIhIYRjAERERESkMAzgiIiIiheGDfE2ZRlPdPVAGPgqRiIgURjEzcBEREQgPD6/ubjxUbm5ukCQJBw4cMMqfNGkSgoODq6dTRERE9NhRTABXU9SqVQvTp0+v7m4QERHRY8wkArhTp06hZ8+eUKvVaNCgAYYMGYJ//vlH3h4bG4tOnTrBwcEBdevWRa9evXDu3DmjOhITE+Hv749atWohICAAmzdvhiRJSE5OBgDExMTAwcHBaJ9bZW73008/oW3btqhVqxbc3d0xe/ZsFBUVVXgsL7/8Mg4cOICtW7dW7iAQERFRjaH4AC4rKwtBQUHw9/fH4cOHERsbi7/++gv9+/eXy+Tm5mLy5MlISkpCXFwczMzM8Oyzz6KkpAQAcO3aNfTu3Ru+vr44evQo5syZc1+zYNu2bcOLL76ICRMm4NSpU/jss88QExODd999t8J1uLm5YfTo0YiKipL7dy/5+fkwGAxGiYiIiEyX4m9iWLZsGdq0aYP33ntPzlu5ciVcXFzwxx9/wNPTE3379jXaZ8WKFahfvz5OnToFHx8frFmzBpIkYfny5ahVqxZatmyJ//73vxg5cmSl+vLuu+/itddew7BhwwAA7u7umDNnDl599VXMnDmzwvW8+eabWLVqFdasWYMhQ4bcs3x0dDRmz55dqb4SERGRcil+Bu7IkSOIj4+HWq2Wk5eXFwDIy6Tnzp3DoEGD4O7uDnt7ezRp0gQAkJGRAQBITU2Fn58fatWqJdfbrl27++rL22+/bdSXkSNHIisrC//++2+F63F0dMTUqVPx1ltvoaCg4J7lo6KioNfr5ZSZmVnpvhMREZFyKH4GrqSkBL1798b7779fapuTkxMAoHfv3nBxccHy5cvh7OyMkpIS+Pj4yMGREKLUtWzijkdLmJmZlcorLCws1ZfZs2fjueeeK9WX24PDipg8eTKWLl2KpUuX3rOsSqWCSqWqVP1ERESkXIoP4Nq0aYMNGzbAzc0NFhalh3P58mWkpKTgs88+Q+fOnQEAe/fuNSrj5eWFNWvWID8/Xw6EDh8+bFTG0dER165dQ25uLmxtbQFAvsHh9r6kpqbCw8PjgcelVqsxY8YMzJo1C717937g+oiIiMh0KGoJVa/XIzk52Si9/PLLuHLlCgYOHIhDhw7h/Pnz2L59O4YPH47i4mLUrl0bdevWxeeff46zZ89i165dmDx5slG9gwYNQklJCUaNGoWUlBRs27YNH330EQDIM3NPPvkkbGxs8Prrr+Ps2bP45ptvEBMTY1TPW2+9ha+++gqzZs3CyZMnkZKSgnXr1uHNN9+8r/GOGjUKGo0G33777X3tT0RERKZJUQFcQkICWrdubZTeeust7Nu3D8XFxQgNDYWPjw8mTpwIjUYDMzMzmJmZYe3atThy5Ah8fHzwyiuv4MMPPzSq197eHj/99BOSk5Ph7++PN954A2+99RaA/1/6rFOnDlavXo2tW7fC19cX3377LWbNmmVUT2hoKLZs2YIdO3bgiSeewFNPPYWPP/4Yrq6u9zVeS0tLzJkzBzdu3Liv/aHX33zLANPdExERkcJI4s4LuwgAsGbNGvznP/+BXq+HtbV1dXenUgwGAzQaDfR6Pezt7au7O0RERFQBlfn9Vvw1cFXlq6++gru7Oxo2bIjjx49j+vTp6N+/v+KCNyIiIjJ9ilpCfZiys7Px4osvokWLFnjllVfw/PPP4/PPP6+y+tesWWP0eJHbk7e3d5W1Q0RERKaPS6iPyLVr1/DXX3+Vuc3S0vK+r5MrC5dQiYiIlIdLqI8hOzs72NnZVXc3iIiIyARwCZWIiIhIYRjAERERESkMAzgiIiIiheE1cKZMo6nuHjw43mNDRERUCmfgHgJJkrB58+bq7gYRERGZqBofwCUmJsLc3BxhYWFVVmdWVhZ69OhRobIM9oiIiKiyanwAt3LlSkRGRmLv3r3IyMiokjq1Wi1UKlWV1EVERER0pxodwOXm5uK7777DmDFj0KtXL8TExMjbrl69isGDB8PR0RHW1tZo1qwZVq1aBQAoKCjA+PHj4eTkhFq1asHNzQ3R0dHyvrfPqt2trJubGwDg2WefhSRJ8ufjx49Dp9PBzs4O9vb2aNu2LQ4fPlzuOPLz82EwGIwSERERma4afRPDunXr0Lx5czRv3hwvvvgiIiMjMWPGDEiShBkzZuDUqVP45ZdfUK9ePZw9exZ5eXkAgEWLFuHHH3/Ed999h8aNGyMzMxOZmZlltnG3sklJSahfvz5WrVqFsLAwmJubAwAGDx6M1q1bY9myZTA3N0dycjIsLS3LHUd0dDRmz55dxUeHiIiIHlc1OoBbsWIFXnzxRQBAWFgYrl+/jri4OISEhCAjIwOtW7dGQEAAgP+fLQOAjIwMNGvWDJ06dYIkSXd9Ddbdyjo6OgIAHBwcoNVqjfaZNm0avLy8AADNmjW76ziioqIwefJk+bPBYICLi0sFjwIREREpTY1dQk1NTcWhQ4fwwgsvAAAsLCwwYMAArFy5EgAwZswYrF27Fv7+/nj11VeRmJgo7xsREYHk5GQ0b94cEyZMwPbt28ttpzJlb5k8eTJeeuklhISEYO7cuTh37txdy6tUKtjb2xslIiIiMl01NoBbsWIFioqK0LBhQ1hYWMDCwgLLli3Dxo0bcfXqVfTo0QMXLlzApEmTcPHiRXTt2hVTp04FALRp0wZpaWmYM2cO8vLy0L9/f/Tr16/MdipT9pZZs2bh5MmTePrpp7Fr1y60bNkSmzZtqvJjQERERMokCVHznpRaVFSERo0a4dVXX0X37t2NtvXt2xeRkZEYP368Uf5nn32GadOmlXmDwLZt2xAWFobLly+jTp06kCQJmzZtQnh4+D3LWllZ4dtvv0Xfvn3L7e/AgQORm5uLH3/8sULjMxgM0Gg00ANQ/Fxczft6EhFRDSX/fuv191xNq5HXwG3ZsgVXr17FiBEjoLnjbQX9+vXDihUrcOnSJbRt2xbe3t7Iz8/Hli1b0KJFCwDA/Pnz4eTkBH9/f5iZmeH777+HVquFg4NDqbbuVdbNzQ1xcXHo2LEjVCoVatWqhWnTpqFfv35o0qQJ/vzzTyQlJd01wCuXXg9wOZWIiMjk1Mgl1BUrViAkJKRU8AbcnIFLTk6GhYUFoqKi4Ofnh8DAQJibm2Pt2rUAALVajffffx8BAQF44oknkJ6ejq1bt8LMrPThvFfZefPmYceOHXBxcUHr1q1hbm6Oy5cvY+jQofD09ET//v3Ro0cP3mVKREREshq5hGrqKjMFS0RERI+Hyvx+18gZOCIiIiIlYwBHREREpDAM4IiIiIgUhgEcERERkcIwgCMiIiJSGAZwRERERArDAI6IiIhIYWrkmxhqjDIeVFyl+AhBIiKiasEZuEqKiIiAJEmQJAkWFhZo3LgxxowZg6tXr1Z314iIiKiGYAB3H8LCwpCVlYX09HR88cUX+OmnnzB27Njq7hYRERHVEAzg7oNKpYJWq0WjRo3QvXt3DBgwANu3bwcABAcHY9KkSUblw8PDERERIX92c3PDe++9h+HDh8POzg6NGzfG559/Lm8vKCjA+PHj4eTkhFq1asHNzQ3R0dGPYmhERESkAAzgHtD58+cRGxsLS0vLSu03b948BAQE4NixYxg7dizGjBmD06dPAwAWLVqEH3/8Ed999x1SU1OxevVquLm5lVtXfn4+DAaDUSIiIiLTxZsY7sOWLVugVqtRXFyMGzduAAA+/vjjStXRs2dPedl1+vTpmD9/PhISEuDl5YWMjAw0a9YMnTp1giRJcHV1vWtd0dHRmD179v0NhoiIiBSHM3D3QafTITk5GQcPHkRkZCRCQ0MRGRlZqTr8/PzkvyVJglarxaVLlwDcvFEiOTkZzZs3x4QJE+Tl2fJERUVBr9fLKTMzs/KDIiIiIsVgAHcfbG1t4eHhAT8/PyxatAj5+fnyDJiZmRnEHY/XKCwsLFXHnUuukiShpKQEANCmTRukpaVhzpw5yMvLQ//+/dGvX79y+6NSqWBvb2+UiIiIyHQxgKsCM2fOxEcffYSLFy/C0dERWVlZ8rbi4mL8/vvvla7T3t4eAwYMwPLly7Fu3Tps2LABV65cqcpuExERkULxGrgqEBwcDG9vb7z33nvo0qULJk+ejJ9//hlNmzbF/PnzkZOTU6n65s+fDycnJ/j7+8PMzAzff/89tFotHBwcHkr/iYiISFkYwFWRyZMn4z//+Q/Onj2L48ePY+jQobCwsMArr7wCnU5XqbrUajXef/99nDlzBubm5njiiSewdetWmJlVcsJUrwe4nEpERGRyJHHnBVukeAaDARqNBnq9ntfDERERKURlfr95DRwRERGRwjCAIyIiIlIYBnBERERECsMAjoiIiEhhGMARERERKQwDOCIiIiKFYQBHREREpDAM4IiIiIgURnEBnCRJ2Lx5c4XLu7m5YcGCBVVeloiIiKi6PDYBXEREBCRJgiRJsLS0RIMGDdCtWzesXLkSJSUlcrmsrCz06NGjwvUmJSVh1KhRVV72QX366aews7NDUVGRnHf9+nVYWlqic+fORmX37NkDSZLwxx9/PJK+ERER0ePtsQngACAsLAxZWVlIT0/HL7/8Ap1Oh4kTJ6JXr15yoKPVaqFSqSpcp6OjI2xsbKq87IPS6XS4fv06Dh8+LOft2bMHWq0WSUlJ+Pfff+X8hIQEODs7w9PT85H0jYiIiB5vj1UAp1KpoNVq0bBhQ7Rp0wavv/46fvjhB/zyyy+IiYkBYLyE2r59e7z22mtGdfz999+wtLREfHw8gNLLorNmzULjxo2hUqng7OyMCRMmyNvuLJuRkYE+ffpArVbD3t4e/fv3x19//WVUl7+/P77++mu4ublBo9HghRdewLVr1+451ubNm8PZ2RkJCQlyXkJCAvr06YOmTZsiMTHRKF+n05VbV35+PgwGg1EiIiIi0/VYBXBl6dKlC1q1aoWNGzeW2jZ48GB8++23EELIeevWrUODBg0QFBRUqvz69esxf/58fPbZZzhz5gw2b94MX1/fMtsVQiA8PBxXrlzB7t27sWPHDpw7dw4DBgwwKnfu3Dls3rwZW7ZswZYtW7B7927MnTu3QmMLDg6WA00AiI+PR3BwMIKCguT8goIC7N+//64BXHR0NDQajZxcXFwq1D4REREp02MfwAGAl5cX0tPTS+UPGDAAFy9exN69e+W8b775BoMGDYKZWemhZWRkQKvVIiQkBI0bN0a7du0wcuTIMtvcuXMnfvvtN3zzzTdo27YtnnzySXz99dfYvXs3kpKS5HIlJSWIiYmBj48POnfujCFDhiAuLq5C4woODsa+fftQVFSEa9eu4dixYwgMDERQUJA8M3fgwAHk5eXdNYCLioqCXq+XU2ZmZoXaJyIiImVSRAAnhIAkSaXyHR0d0a1bN6xZswYAkJaWhv3792Pw4MFl1vP8888jLy8P7u7uGDlyJDZt2mR0E8HtUlJS4OLiYjSb1bJlSzg4OCAlJUXOc3Nzg52dnfzZyckJly5dqtC4dDodcnNzkZSUhD179sDT0xP169dHUFAQkpKSkJubi4SEBDRu3Bju7u7l1qNSqWBvb2+UiIiIyHQpIoBLSUlBkyZNytw2ePBgrF+/HoWFhfjmm2/g7e2NVq1alVnWxcUFqampWLJkCaytrTF27FgEBgaisLCwVNnygsY78y0tLY22S5JkdNfs3Xh4eKBRo0aIj49HfHy8vOyr1WrRpEkT7Nu3D/Hx8ejSpUuF6iMiIqKa4bEP4Hbt2oUTJ06gb9++ZW4PDw/HjRs3EBsbi2+++QYvvvjiXeuztrbGM888g0WLFiEhIQH79+/HiRMnSpVr2bIlMjIyjJYjT506Bb1ejxYtWjzYoG6j0+mQkJCAhIQEBAcHy/lBQUHYtm0bDhw4cNflUyIiIqp5LKq7A7fLz89HdnY2iouL8ddffyE2NhbR0dHo1asXhg4dWuY+tra26NOnD2bMmIGUlBQMGjSo3PpjYmJQXFyMJ598EjY2Nvj6669hbW0NV1fXUmVDQkLg5+eHwYMHY8GCBSgqKsLYsWMRFBSEgICAKhuzTqfDuHHjUFhYaHTjRVBQEMaMGYMbN24wgCMiIiIjj9UMXGxsLJycnODm5oawsDDEx8dj0aJF+OGHH2Bubl7ufoMHD8bx48fRuXNnNG7cuNxyDg4OWL58OTp27Ag/Pz/ExcXhp59+Qt26dUuVvfW4ktq1ayMwMBAhISFwd3fHunXrqmSst+h0OuTl5cHDwwMNGjSQ84OCgnDt2jU0bdqUd5USERGREUnc/gwOMgkGgwEajQZ6vZ43NBARESlEZX6/H6sZOCIiIiK6NwZwD0lGRgbUanW5KSMjo7q7SERERAr1WN3EYEqcnZ2RnJx81+1ERERE94MB3ENiYWEBDw+P6u4GERERmSAuoRIREREpDAM4IiIiIoVhAEdERESkMAzgiIiIiBSmRgVwERERCA8PL5WfkJAASZKQk5Nz33UHBwdDkiRIkgQrKys0bdoUUVFRyM/Pv/8OExEREZWBd6FWoZEjR+Ltt99GQUEBkpKS8J///AcAEB0dXc09IyIiIlNSo2bgKiImJgYODg7YsmULmjdvDhsbG/Tr1w+5ubn48ssv4ebmhtq1ayMyMhLFxcVG+9rY2ECr1aJx48bo27cvunXrhu3bt8vb3dzcsGDBAqN9/P39MWvWLPmzJEn44osv8Oyzz8LGxgbNmjXDjz/++DCHTERERArDAK4M//77LxYtWoS1a9ciNjYWCQkJeO6557B161Zs3boVX3/9NT7//HOsX7++3DqOHz+Offv2wdLSstLtz549G/3798dvv/2Gnj17YvDgwbhy5Uq55fPz82EwGIwSERERma4aF8Bt2bKl1GutevToYVSmsLAQy5YtQ+vWrREYGIh+/fph7969WLFiBVq2bIlevXpBp9MhPj7eaL+lS5dCrVZDpVLB398ff//9N6ZNm1bpPkZERGDgwIHw8PDAe++9h9zcXBw6dKjc8tHR0dBoNHJycXGpdJtERESkHDUugNPpdEhOTjZKX3zxhVEZGxsbNG3aVP7coEEDuLm5Qa1WG+VdunTJaL/BgwcjOTkZ+/fvR//+/TF8+HD07du30n308/OT/7a1tYWdnV2ptm4XFRUFvV4vp8zMzEq3SURERMpR425isLW1LfWKqz///NPo853LnpIklZlXUlJilKfRaOS6V69eDW9vb6xYsQIjRowAAJiZmUEIYbRPYWFhqT5WpK3bqVQqqFSqcrcTERGRaalxM3CPiqWlJV5//XW8+eab+PfffwEAjo6OyMrKkssYDAakpaVVVxeJiIhIoRjAPUSDBg2CJElYunQpAKBLly74+uuvsWfPHvz+++8YNmwYzM3Nq7mXREREpDQM4B4iKysrjB8/Hh988AGuX7+OqKgoBAYGolevXujZsyfCw8ONrrUjIiIiqghJ3HlRFimewWCARqOBXq+Hvb19dXeHiIiIKqAyv9+cgSMiIiJSGAZwRERERArDAI6IiIhIYRjAERERESkMAzgiIiIihWEAR0RERKQwDOCIiIiIFIYBHBEREZHCVCqAi4iIgCRJmDt3rlH+5s2bIUlSlXbslpiYGDg4ODyUuh8n1XFsiYiISJkqPQNXq1YtvP/++7h69erD6E+NxmNLREREFVHpAC4kJARarRbR0dHllklMTERgYCCsra3h4uKCCRMmIDc3FwCwePFi+Pr6ymVvzTAtWbJEzgsNDUVUVFSF+qPX6zFq1CjUr18f9vb26NKlC44fPy5vP3fuHPr06YMGDRpArVbjiSeewM6dO43qyMrKwtNPPw1ra2s0adIE33zzDdzc3LBgwQIAQHp6OiRJQnJysrxPTk4OJElCQkKCnHfq1Cn07NkTarUaDRo0wJAhQ/DPP/9UaBxAxY4tERERUaUDOHNzc7z33ntYvHgx/vzzz1LbT5w4gdDQUDz33HP47bffsG7dOuzduxfjx48HAAQHB+PkyZNyYLN7927Uq1cPu3fvBgAUFRUhMTERQUFB9+yLEAJPP/00srOzsXXrVhw5cgRt2rRB165dceXKFQDA9evX0bNnT+zcuRPHjh1DaGgoevfujYyMDLmeoUOH4uLFi0hISMCGDRvw+eef49KlS5U6LllZWQgKCoK/vz8OHz6M2NhY/PXXX+jfv3+F67jXsS1Pfn4+DAaDUSIiIiITJiph2LBhok+fPkIIIZ566ikxfPhwIYQQmzZtEreqGjJkiBg1apTRfnv27BFmZmYiLy9PlJSUiHr16on169cLIYTw9/cX0dHRon79+kIIIRITE4WFhYW4du2aEEKIVatWCY1GU2Z/4uLihL29vbhx44ZRftOmTcVnn31W7jhatmwpFi9eLIQQIiUlRQAQSUlJ8vYzZ84IAGL+/PlCCCHS0tIEAHHs2DG5zNWrVwUAER8fL4QQYsaMGaJ79+5G7WRmZgoAIjU1tdy+3FKRY1uemTNnCgClkl6vv2e7RERE9HjQ6/UV/v2+77tQ33//fXz55Zc4deqUUf6RI0cQExMDtVotp9DQUJSUlCAtLQ2SJCEwMBAJCQnIycnByZMnMXr0aBQXFyMlJQUJCQlo06YN1Gr1Pftw5MgRXL9+HXXr1jVqLy0tDefOnQMA5Obm4tVXX0XLli3h4OAAtVqN06dPyzNwqampsLCwQJs2beR6PTw8ULt27UodjyNHjiA+Pt6oH15eXgAg96Wiyju25YmKioJer5dTZmZmpdojIiIiZbG43x0DAwMRGhqK119/HREREXJ+SUkJXn75ZUyYMKHUPo0bNwZwcxn1888/x549e9CqVSs4ODggMDAQu3fvRkJCAoKDgyvUh5KSEjg5ORldh3bLrTtXp02bhm3btuGjjz6Ch4cHrK2t0a9fPxQUFAC4uQxbltvzzczMSuUVFhaW6kvv3r3x/vvvl6rLycmpQuO5pbxjWx6VSgWVSlWpNoiIiEi57juAA4C5c+fC398fnp6ecl6bNm1w8uRJeHh4lLtfcHAwJk6ciPXr18vBWlBQEHbu3InExERMnDixQu23adMG2dnZsLCwgJubW5ll9uzZg4iICDz77LMAbl4Tl56eLm/38vJCUVERjh07hrZt2wIAzp49i5ycHLmMo6MjgJvXubVu3RoAjG5ouNWXDRs2wM3NDRYWD3RYAZR9bImIiIiAB3yQr6+vLwYPHozFixfLedOnT8f+/fsxbtw4JCcn48yZM/jxxx8RGRkpl/Hx8UHdunWxZs0aOYALDg7G5s2bkZeXh06dOhm1U1xcjOTkZKN06tQphISEoH379ggPD8e2bduQnp6OxMREvPnmmzh8+DCAm8uhGzduRHJyMo4fP45BgwahpKRErtvLywshISEYNWoUDh06hGPHjmHUqFGwtraWn79mbW2Np556CnPnzsWpU6fw66+/4s033zTq47hx43DlyhUMHDgQhw4dwvnz57F9+3YMHz4cxcXFVXJsiYiIiIAqeBPDnDlzjJYW/fz8sHv3bpw5cwadO3dG69atMWPGDKNlREmS5LtMO3fuLO+n0WjQunVr2NvbG7Vx/fp1tG7d2ij17NkTkiRh69atCAwMxPDhw+Hp6YkXXngB6enpaNCgAQBg/vz5qF27Njp06IDevXsjNDTU6Ho3APjqq6/QoEEDBAYG4tlnn8XIkSNhZ2eHWrVqyWVWrlyJwsJCBAQEYOLEiXjnnXeM6nB2dsa+fftQXFyM0NBQ+Pj4YOLEidBoNPIS7IMeWyIiIiIAkAQjhFL+/PNPuLi4YOfOnejatWt1d6fSDAYDNBoN9Hp9qWCYiIiIHk+V+f1+8Iu1TMCuXbtw/fp1+Pr6IisrC6+++irc3NwQGBhY3V0jIiIiKoUBHG7eUfr666/j/PnzsLOzQ4cOHbBmzRpYWlpWSf0ZGRlo2bJludtPnTol36FLREREdC9cQn0EioqKjO58vVNV3bl6i16vh4ODAzIzM7mESkREpBAGgwEuLi7IycmBRqO5a1nOwD0CFhYWd32sSlW7fPkyAMDFxeWRtUlERERV49q1awzgaqI6deoAuLl0e68vgCm59f9catrMY00dN1Bzx85xc9w1QU0ctxAC165dg7Oz8z3LMoAzQbceW6LRaGrMl/529vb2HHcNU1PHznHXLBx3zVDRiZcHfg4cERERET1aDOCIiIiIFIYBnAlSqVSYOXNmjXvBPcdds8YN1Nyxc9wcd01QU8ddUXyMCBEREZHCcAaOiIiISGEYwBEREREpDAM4IiIiIoVhAEdERESkMAzgTNDSpUvRpEkT1KpVC23btsWePXuqu0v3LTo6Gk888QTs7OxQv359hIeHIzU11ahMREQEJEkySk899ZRRmfz8fERGRqJevXqwtbXFM888gz///PNRDqVSZs2aVWpMWq1W3i6EwKxZs+Ds7Axra2sEBwfj5MmTRnUobczAzfcC3zluSZIwbtw4AKZ1rn/99Vf07t0bzs7OkCQJmzdvNtpeVef46tWrGDJkCDQaDTQaDYYMGYKcnJyHPLry3W3chYWFmD59Onx9fWFrawtnZ2cMHToUFy9eNKojODi41PfghRdeMCqjpHEDVffdVtq4y/r3LkkSPvzwQ7mMEs/3o8AAzsSsW7cOkyZNwhtvvIFjx46hc+fO6NGjBzIyMqq7a/dl9+7dGDduHA4cOIAdO3agqKgI3bt3R25urlG5sLAwZGVlyWnr1q1G2ydNmoRNmzZh7dq12Lt3L65fv45evXqhuLj4UQ6nUry9vY3GdOLECXnbBx98gI8//hiffPIJkpKSoNVq0a1bN1y7dk0uo8QxJyUlGY15x44dAIDnn39eLmMq5zo3NxetWrXCJ598Uub2qjrHgwYNQnJyMmJjYxEbG4vk5GQMGTLkoY+vPHcb97///oujR49ixowZOHr0KDZu3Ig//vgDzzzzTKmyI0eONPoefPbZZ0bblTTuW6riu620cd8+3qysLKxcuRKSJKFv375G5ZR2vh8JQSalXbt2YvTo0UZ5Xl5e4rXXXqumHlWtS5cuCQBi9+7dct6wYcNEnz59yt0nJydHWFpairVr18p5//3vf4WZmZmIjY19mN29bzNnzhStWrUqc1tJSYnQarVi7ty5ct6NGzeERqMRn376qRBCmWMuy8SJE0XTpk1FSUmJEMI0z7UQQgAQmzZtkj9X1Tk+deqUACAOHDggl9m/f78AIE6fPv2QR3Vvd467LIcOHRIAxIULF+S8oKAgMXHixHL3UeK4q+K7rcRx36lPnz6iS5cuRnlKP98PC2fgTEhBQQGOHDmC7t27G+V3794diYmJ1dSrqqXX6wEAderUMcpPSEhA/fr14enpiZEjR+LSpUvytiNHjqCwsNDouDg7O8PHx+exPi5nzpyBs7MzmjRpghdeeAHnz58HAKSlpSE7O9toPCqVCkFBQfJ4lDrm2xUUFGD16tUYPnw4JEmS803xXN+pqs7x/v37odFo8OSTT8plnnrqKWg0GsUcD71eD0mS4ODgYJS/Zs0a1KtXD97e3pg6darRzKRSx/2g322ljvuWv/76Cz///DNGjBhRapspnu8HxZfZm5B//vkHxcXFaNCggVF+gwYNkJ2dXU29qjpCCEyePBmdOnWCj4+PnN+jRw88//zzcHV1RVpaGmbMmIEuXbrgyJEjUKlUyM7OhpWVFWrXrm1U3+N8XJ588kl89dVX8PT0xF9//YV33nkHHTp0wMmTJ+U+l3WeL1y4AACKHPOdNm/ejJycHERERMh5pniuy1JV5zg7Oxv169cvVX/9+vUVcTxu3LiB1157DYMGDTJ6mfngwYPRpEkTaLVa/P7774iKisLx48flJXcljrsqvttKHPftvvzyS9jZ2eG5554zyjfF810VGMCZoNtnK4Cbgc+deUo0fvx4/Pbbb9i7d69R/oABA+S/fXx8EBAQAFdXV/z888+l/kNwu8f5uPTo0UP+29fXF+3bt0fTpk3x5Zdfyhc23895fpzHfKcVK1agR48ecHZ2lvNM8VzfTVWc47LKK+F4FBYW4oUXXkBJSQmWLl1qtG3kyJHy3z4+PmjWrBkCAgJw9OhRtGnTBoDyxl1V322ljft2K1euxODBg1GrVi2jfFM831WBS6gmpF69ejA3Ny/1/zguXbpU6v/JK01kZCR+/PFHxMfHo1GjRnct6+TkBFdXV5w5cwYAoNVqUVBQgKtXrxqVU9JxsbW1ha+vL86cOSPfjXq386z0MV+4cAE7d+7ESy+9dNdypniuAVTZOdZqtfjrr79K1f/3338/1sejsLAQ/fv3R1paGnbs2GE0+1aWNm3awNLS0uh7oMRx3+5+vttKHveePXuQmpp6z3/zgGme7/vBAM6EWFlZoW3btvK08i07duxAhw4dqqlXD0YIgfHjx2Pjxo3YtWsXmjRpcs99Ll++jMzMTDg5OQEA2rZtC0tLS6PjkpWVhd9//10xxyU/Px8pKSlwcnKSlxJuH09BQQF2794tj0fpY161ahXq16+Pp59++q7lTPFcA6iyc9y+fXvo9XocOnRILnPw4EHo9frH9njcCt7OnDmDnTt3om7duvfc5+TJkygsLJS/B0oc953u57ut5HGvWLECbdu2RatWre5Z1hTP932pjjsn6OFZu3atsLS0FCtWrBCnTp0SkyZNEra2tiI9Pb26u3ZfxowZIzQajUhISBBZWVly+vfff4UQQly7dk1MmTJFJCYmirS0NBEfHy/at28vGjZsKAwGg1zP6NGjRaNGjcTOnTvF0aNHRZcuXUSrVq1EUVFRdQ3trqZMmSISEhLE+fPnxYEDB0SvXr2EnZ2dfB7nzp0rNBqN2Lhxozhx4oQYOHCgcHJyUvSYbykuLhaNGzcW06dPN8o3tXN97do1cezYMXHs2DEBQHz88cfi2LFj8t2WVXWOw8LChJ+fn9i/f7/Yv3+/8PX1Fb169Xrk473lbuMuLCwUzzzzjGjUqJFITk42+jefn58vhBDi7NmzYvbs2SIpKUmkpaWJn3/+WXh5eYnWrVsrdtxV+d1W0rhv0ev1wsbGRixbtqzU/ko9348CAzgTtGTJEuHq6iqsrKxEmzZtjB65oTQAykyrVq0SQgjx77//iu7duwtHR0dhaWkpGjduLIYNGyYyMjKM6snLyxPjx48XderUEdbW1qJXr16lyjxOBgwYIJycnISlpaVwdnYWzz33nDh58qS8vaSkRMycOVNotVqhUqlEYGCgOHHihFEdShvzLdu2bRMARGpqqlG+qZ3r+Pj4Mr/bw4YNE0JU3Tm+fPmyGDx4sLCzsxN2dnZi8ODB4urVq49olKXdbdxpaWnl/puPj48XQgiRkZEhAgMDRZ06dYSVlZVo2rSpmDBhgrh8+bJRO0oad1V+t5U07ls+++wzYW1tLXJyckrtr9Tz/ShIQgjxUKf4iIiIiKhK8Ro4IiIiIoVhAEdERESkMAzgiIiIiBSGARwRERGRwjCAIyIiIlIYBnBERERECsMAjoiIiEhhGMARERERKQwDOCKqsSRJwubNmytcftasWfD3979rmYiICISHhz9Qv4iI7oUBHBE9tnr37o2QkJAyt+3fvx+SJOHo0aP3XX9WVhZ69Ohx3/s/LMHBwZg0aVJ1d6NcCQkJkCQJOTk51d0VohqLARwRPbZGjBiBXbt24cKFC6W2rVy5Ev7+/mjTpk2l6y0oKAAAaLVaqFSqB+5nTVJYWFjdXSAiMIAjosdYr169UL9+fcTExBjl//vvv1i3bh1GjBiBy5cvY+DAgWjUqBFsbGzg6+uLb7/91qh8cHAwxo8fj8mTJ6NevXro1q0bgNJLqNOnT4enpydsbGzg7u6OGTNmlBmwfPbZZ3BxcYGNjQ2ef/75u85ECSHwwQcfwN3dHdbW1mjVqhXWr19fqePg5uaGd955B0OHDoVarYarqyt++OEH/P333+jTpw/UajV8fX1x+PBheZ+YmBg4ODhg8+bN8PT0RK1atdCtWzdkZmYa1b1s2TI0bdoUVlZWaN68Ob7++muj7ZIk4dNPP0WfPn1ga2uLl156CTqdDgBQu3ZtSJKEiIgIAEBsbCw6deoEBwcH1K1bF7169cK5c+fkutLT0yFJEjZu3AidTgcbGxu0atUK+/fvN2pz3759CAoKgo2NDWrXro3Q0FBcvXq1yo4nkUmoxIvviYgeuWnTpgk3NzdRUlIi58XExAiVSiWuXLki/vzzT/Hhhx+KY8eOiXPnzolFixYJc3NzceDAAbl8UFCQUKvVYtq0aeL06dMiJSVFCCEEALFp0ya53Jw5c8S+fftEWlqa+PHHH0WDBg3E+++/L2+fOXOmsLW1FV26dBHHjh0Tu3fvFh4eHmLQoEFymWHDhok+ffrIn19//XXh5eUlYmNjxblz58SqVauESqUSCQkJ5Y45KChITJw4Uf7s6uoq6tSpIz799FPxxx9/iDFjxgg7OzsRFhYmvvvuO5GamirCw8NFixYt5OO0atUqYWlpKQICAkRiYqI4fPiwaNeunejQoYNc78aNG4WlpaVYsmSJSE1NFfPmzRPm5uZi165dchkAon79+mLFihXi3LlzIj09XWzYsEEAEKmpqSIrK0vk5OQIIYRYv3692LBhg/jjjz/EsWPHRO/evYWvr68oLi4WQgiRlpYmAAgvLy+xZcsWkZqaKvr16ydcXV1FYWGhEEKIY8eOCZVKJcaMGSOSk5PF77//LhYvXiz+/vvv+z6eRKaIARwRPdZSUlIEAKOgIjAwUAwcOLDcfXr27CmmTJkifw4KChL+/v6lyt0ZwN3pgw8+EG3btpU/z5w5U5ibm4vMzEw575dffhFmZmYiKytLCGEcwF2/fl3UqlVLJCYmGtU7YsSIu/a/rADuxRdflD9nZWUJAGLGjBly3v79+wUAuR+rVq0SAIwC2VvH8uDBg0IIITp06CBGjhxp1Pbzzz8vevbsKX8GICZNmmRUJj4+XgAQV69eLXcMQghx6dIlAUCcOHFCCPH/AdwXX3whlzl58qQAIAfVAwcOFB07diyzvvs9nkSmiEuoRPRY8/LyQocOHbBy5UoAwLlz57Bnzx4MHz4cAFBcXIx3330Xfn5+qFu3LtRqNbZv346MjAyjegICAu7Z1vr169GpUydotVqo1WrMmDGjVD2NGzdGo0aN5M/t27dHSUkJUlNTS9V36tQp3LhxA926dYNarZbTV199ZbS0WBF+fn7y3w0aNAAA+Pr6lsq7dOmSnGdhYWE0bi8vLzg4OCAlJQUAkJKSgo4dOxq107FjR3n7LRU5dsDNczNo0CC4u7vD3t4eTZo0AYBSx/D2sTg5ORn1Ozk5GV27di2z/qo8nkRKZ1HdHSAiupcRI0Zg/PjxWLJkCVatWgVXV1f5R37evHmYP38+FixYAF9fX9ja2mLSpEnyjQq32Nra3rWNAwcO4IUXXsDs2bMRGhoKjUaDtWvXYt68eXfdT5Iko/+9XUlJCQDg559/RsOGDY22VfbmCUtLy1JtlpV3q80788vLu3O7EKJU3r2O3S29e/eGi4sLli9fDmdnZ5SUlMDHx6fUubhbv62trcutvyqPJ5HScQaOiB57/fv3h7m5Ob755ht8+eWX+M9//iP/8O/Zswd9+vTBiy++iFatWsHd3R1nzpypdBv79u2Dq6sr3njjDQQEBKBZs2Zl3v2akZGBixcvyp/3798PMzMzeHp6lirbsmVLqFQqZGRkwMPDwyi5uLhUuo+VVVRUZHRjQ2pqKnJycuDl5QUAaNGiBfbu3Wu0T2JiIlq0aHHXeq2srADcnP285fLly0hJScGbb76Jrl27okWLFvKNB5Xh5+eHuLi4MrdV9/EkepxwBo6IHntqtRoDBgzA66+/Dr1eL9/1CAAeHh7YsGEDEhMTUbt2bXz88cfIzs6+ZxByJw8PD2RkZGDt2rV44okn8PPPP2PTpk2lytWqVQvDhg3DRx99BIPBgAkTJqB///7QarWlytrZ2WHq1Kl45ZVXUFJSgk6dOsFgMCAxMRFqtRrDhg2r9LGoDEtLS0RGRmLRokWwtLTE+PHj8dRTT6Fdu3YAgGnTpqF///5o06YNunbtip9++gkbN27Ezp0771qvq6srJEnCli1b0LNnT1hbW6N27dqoW7cuPv/8czg5OSEjIwOvvfZapfscFRUFX19fjB07FqNHj4aVlRXi4+Px/PPPo169etV6PIkeJ5yBIyJFGDFiBK5evYqQkBA0btxYzp8xYwbatGmD0NBQBAcHQ6vV3tebEPr06YNXXnkF48ePh7+/PxITEzFjxoxS5Tw8PPDcc8+hZ8+e6N69O3x8fLB06dJy650zZw7eeustREdHo0WLFggNDcVPP/0kXx/2MNnY2GD69OkYNGgQ2rdvD2tra6xdu1beHh4ejoULF+LDDz+Et7c3PvvsM6xatQrBwcF3rbdhw4aYPXs2XnvtNTRo0ADjx4+HmZkZ1q5diyNHjsDHxwevvPIKPvzww0r32dPTE9u3b8fx48fRrl07tG/fHj/88AMsLG7ON1Tn8SR6nEhCCFHdnSAioqoVExODSZMm8W0JRCaKM3BERERECsMAjoiIiEhhuIRKREREpDCcgSMiIiJSGAZwRERERArDAI6IiIhIYRjAERERESkMAzgiIiIihWEAR0RERKQwDOCIiIiIFIYBHBEREZHC/B80iEUmNeKKOQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Importance.sort_values(by = 'Importance', axis = 0 , ascending=True).plot(kind='barh', color='r')\n",
    "plt.xlabel('Variable Importance')\n",
    "plt.gca().legend_ = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cab76c06-bbc4-40da-bc77-7722f7daa84f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.86141353,  4.32251058,  3.59488159,  4.5849566 ,  4.55157582,\n",
       "        5.57909803,  6.98254942,  6.61501666,  9.83943806,  5.00727323,\n",
       "        8.93188501, 10.00533385,  5.10570429,  7.76594432,  4.62295354,\n",
       "        3.93314061,  0.82732859,  0.98026126,  0.88873502])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbm_model_tuned.feature_importances_*100"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
